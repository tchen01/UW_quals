\documentclass[12pt]{article}
\usepackage[T1]{fontenc}

% Document Details
\newcommand{\CLASS}{Numerics}
\newcommand{\assigmentnum}{Methods and Useful Facts}


\usepackage[margin = 1in, top = 1.25in, bottom = 1.in]{geometry}
\input{../../TeX_headers/title.tex} % Title Styling
\input{../../TeX_headers/sfftoc.tex} % ToC Styling
\input{../../TeX_headers/styling.tex} % General Styling
\input{../../TeX_headers/section.tex} % Section Styling
\input{../../TeX_headers/code.tex} % Code Display Setup
\input{../../TeX_headers/math.tex} % Math shortcuts
\input{../../TeX_headers/problem.tex} % Math shortcuts
\input{../../TeX_headers/proof.tex} % Math shortcuts


\hypersetup{
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=violet,          % color of internal links (change box color with linkbordercolor)
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\setlength{\headheight}{15pt}
\newcommand{\note}[1]{\textcolor{red}{\textit{Note:} #1}}

% overwrite old problem class to be able to add to ToC
\let\savedprob=\problem%
\def\problem[#1]{\pagebreak\phantomsection\addcontentsline{toc}{subsection}{#1}\savedprob[#1]\label{#1}}


\begin{document}
\maketitle

\pagebreak
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%
%    Useful Info    %
%%%%%%%%%%%%%%%%%%%%%

\pagebreak
\section{Calculus}

\subsection{Gradient and Jacobian}
For \( f:\RR^n\to\RR \) we define the gradient as,
\begin{align*}
    \nabla f = \left[ \pp{f}{x_1}, \pp{f}{x_2}, \ldots, \pp{f}{x_n} \right]
\end{align*}

For \( f:\RR^n\to\RR^m \) we define the Jacobian as,
\begin{align*}
    J_f = \left[\begin{array}{c}
        \nabla f_1 \\ \nabla f_2 \\ \vdots \\ \nabla f_m
    \end{array}\right]
    =
    \left[\begin{array}{cccc}
        \pp{f_1}{x_1} & \pp{f_1}{x_2} & \cdots &\pp{f_1}{x_n} \\
        \pp{f_2}{x_1} & \pp{f_2}{x_2} & \cdots &\pp{f_2}{x_n} \\
        \vdots & \vdots & & \vdots\\
         \pp{f_m}{x_1} & \pp{f_m}{x_2} &\ldots & \pp{f_m}{x_n}
    \end{array}\right]
\end{align*}

Note that a liner approximation to \( f \) at \( x_0 \) is given by,
\begin{align*}
    f(x_0) + J_f x
\end{align*}


\subsection{Gradients of Matrix Vector Forms}
\begin{align*}
    \nabla b^TAx &= A^Tb \\
    \nabla x^TAx &= (A+A^T)x
\end{align*}

\subsection{Taylor Expansions}

2d case.

\subsubsection{Computing Expansions in Mathematica}



\pagebreak
\section{Matrices}

\subsection{Classification of Matrices}
Matrices are assumed to be complex unless specified otherwise.

\subsubsection{Hermetian}
\textit{Definition}: A matrix \( A \) is Hermetian if \( A^* = A \)

\textit{Properties}:
\begin{itemize}[nolistsep]
    \item[\(\Rightarrow\)] Real eigenvalues
    \item[\(\Rightarrow\)] \hyperref[sec:normal]{Normal}
    \item[\(\Rightarrow\)] Unitarily diagonalizable
    \item[\(\Rightarrow\)] \( \norm{A}_2 = \rho(A) \)
\end{itemize}


\subsubsection{Skew symmetric}
\textit{Definition}: A real matrix \( A \) is skew symmetric if \( A^T = -A \)

\textit{Properties}:
\begin{itemize}[nolistsep]
    \item[\(\Rightarrow\)] pure imaginary eigenvalues
    \item[\(\Rightarrow\)] \( I+A \) is invertible
\end{itemize}


\subsubsection{Normal}
\label{sec:normal}
\textit{Definition}: A matrix \( A \) is normal if \( A^*A = AA^* \)

\textit{Properties}:
\begin{itemize}[nolistsep]
    \item[\(\Leftrightarrow\)] Unitarily diagonalizable  
\end{itemize}


\subsubsection{Positive definite}
\textit{Definition}: A matrix \( A \) is positive definite if all eigenvalues are positive.

\textit{Properties}:
\begin{itemize}[nolistsep]
    \item[\(\Leftrightarrow\)] \( v^*Av > 0 \) for all \( v \)
    \item[\(\Leftrightarrow\)] Has \hyperref[sec:cholesky]{Cholesky} factorization
\end{itemize}


\subsection{???}
Where should I put things like similarity transform, unitarily diagonalizable, etc.

Also things like 


\subsection{Matrix Decompositions}
\subsubsection{SVD}
\begin{align*}
    A = U\Sigma V^*
\end{align*}

\begin{itemize}[nolistsep]
    \item \( U \) unitary
    \item \( \Sigma \) diagonal, with real positive entries in non-increasing order
    \item \( V \) unitary
\end{itemize}

\textit{Existence}:
Always

\textit{Uniqueness}:
\note{double check}
Unique up to complex sign of columns of \( U \) and \( V \)

\textit{Computing}:


\textit{Why it is useful}:
\begin{itemize}[nolistsep]
    \item Rank revealing
    \item Numerical stability of algorithms using SVD
\end{itemize}


\subsubsection{(P)LU}

\textit{Existence}:

\textit{Uniqueness}:

\textit{Computing}:
Gaussian Elimination 

When is pivoting needed?



\subsubsection{Cholesky}
\label{sec:cholesky}
\textit{Existence}: If \( A \) is Hermetian positive definite

\textit{Uniqueness}: Unique up to sign

\textit{Why it is useful}:
\begin{itemize}[nolistsep]
    \item Save storage space
\end{itemize}

\subsubsection{QR}


\subsubsection{Eigen}


% these are less relevant
\subsubsection{Shur}

\subsubsection{Jordan Normal}

\subsection{Matrix and Vector Norms}

\begin{align*}
    \norm{A} = \sup_{u\neq 0} \frac{\norm{Au}}{\norm{u}} = \sup_{\norm{u}=1} \norm{Au}
\end{align*}

\begin{align*}
    \norm{A} = \sup_{u,v\neq 0} \frac{\ip{Au,v}}{\norm{u}\norm{v}} = \sup_{\norm{u}=\norm{v}=1} \ip{Au,v}
\end{align*}
 
If \( A \) is self Hermetian,
\note{double check this}
\begin{align*}
    \norm{A} = \sup_{u\neq 0} \frac{\ip{Au,u}}{\norm{u}^2} = \sup_{\norm{u}=1} \ip{Au,u}
\end{align*}

\subsection{Rayleigh Quotients}
\note{Anne seems to like these}



\section{Linear Systems}

\subsection{Direct Methods}
\subsubsection{QR}

\subsubsection{Gaussian Elimination}

\subsubsection{SVD}



\subsection{Iterative Metods}
\subsubsection{Simple Iteration}

\subsubsection{Conjugate Gradient}

\subsubsection{GMRES}


\subsection{Solving Least Squares}
The linear least squares problem is,
\begin{align*}
    \min_{x} \norm{b-Ax}_2
\end{align*}

This is solved when \( x \) solve the linear system (called the normal equations),
\begin{align*}
    A^TAx = A^Tb
\end{align*}


\subsubsection{Derivations of Normal Equations}

\textit{Using Projectors}:


\textit{Using Calculus}:

Note that
\begin{align*}
    \norm{b-Ax} = (b-Ax)^T(b-Ax) = b^Tb + -2b^TAx + x^T(A^TA)x
\end{align*}
Therefore, taking the gradient of \( \norm{b-Ax} \), we know it is minimized when \( 2A^TAx - 2A^Tb = 0 \). 

\note{How do we do derivative of things like x*Ax }

\subsubsection{Solving Least Squares Numerically}

\pagebreak
\section{Boundary Value Problems}

\subsection{Laplacian}


\pagebreak
\section{Initial Value Problems}
\note{need better section names}

\subsection{Runge-Kutta Methods}


\subsection{LMMs}


\subsection{Stability}


zero stable

region of abs stability

A stable L stable etc

MOL

Von Neumann



\end{document}

