\documentclass[12pt]{article}
\usepackage[T1]{fontenc}

% Document Details
\newcommand{\CLASS}{Numerics}
\newcommand{\assigmentnum}{Methods and Useful Facts}


\usepackage[margin = 1in, top = 1.25in, bottom = 1.in]{geometry}
\input{../../TeX_headers/title.tex} % Title Styling
\input{../../TeX_headers/sfftoc.tex} % ToC Styling
\input{../../TeX_headers/styling.tex} % General Styling
\input{../../TeX_headers/section.tex} % Section Styling
\input{../../TeX_headers/code.tex} % Code Display Setup
\input{../../TeX_headers/math.tex} % Math shortcuts
\input{../../TeX_headers/problem.tex} % Math shortcuts
\input{../../TeX_headers/proof.tex} % Math shortcuts


\hypersetup{
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=violet,          % color of internal links (change box color with linkbordercolor)
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\setlength{\headheight}{15pt}
\newcommand{\note}[1]{\textcolor{red}{\textit{Note:} #1}}

% overwrite old problem class to be able to add to ToC
\let\savedprob=\problem%
\def\problem[#1]{\pagebreak\phantomsection\addcontentsline{toc}{subsection}{#1}\savedprob[#1]\label{#1}}


\begin{document}
\maketitle

\pagebreak
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%
%    Useful Info    %
%%%%%%%%%%%%%%%%%%%%%

\pagebreak
\section{Calculus}

\subsection{Gradient and Jacobian}
For \( f:\RR^n\to\RR \) we define the gradient as,
\begin{align*}
    \nabla f = \left[ \pp{f}{x_1}, \pp{f}{x_2}, \ldots, \pp{f}{x_n} \right]
\end{align*}

For \( f:\RR^n\to\RR^m \) we define the Jacobian as,
\begin{align*}
    J_f = \left[\begin{array}{c}
        \nabla f_1 \\ \nabla f_2 \\ \vdots \\ \nabla f_m
    \end{array}\right]
    =
    \left[\begin{array}{cccc}
        \pp{f_1}{x_1} & \pp{f_1}{x_2} & \cdots &\pp{f_1}{x_n} \\
        \pp{f_2}{x_1} & \pp{f_2}{x_2} & \cdots &\pp{f_2}{x_n} \\
        \vdots & \vdots & & \vdots\\
         \pp{f_m}{x_1} & \pp{f_m}{x_2} &\ldots & \pp{f_m}{x_n}
    \end{array}\right]
\end{align*}

Note that the best linear approximation to \( f \) at \( x_0 \) is given by,
\begin{align*}
    f(x_0) + J_f x
\end{align*}


\subsection{Gradients of Matrix Vector Forms}
This can be useful for minimizing/maximizing expressions involving matrix vector quantities.
\begin{align*}
    \nabla b^TAx &= A^Tb \\
    \nabla x^TAx &= (A+A^T)x
\end{align*}


\subsection{Taylor Expansions}
\begin{align*}
    f(t+k,x+h) = f + kf_t + hf_x + \frac{k^2}{2}f_{tt} + kh f_{tx} + \frac{h^2}{2} f_{xx} + \cO(k^2+h^2)
\end{align*}


\subsubsection{Computing Expansions in Mathematica}
Compute Taylor expansion of \( f(t+k,x+h) \) to \( d \)-th order.
\begin{lstlisting}
Normal[Series[f[t + z k, x + z h], {z,0,d}]] /. {z->1}
\end{lstlisting}

This can be written into a function like,
\begin{lstlisting}
F[n_,j_] := Normal[Series[f[t + z n k, x + z j h], {z,0,d}]] /. {z->1}
\end{lstlisting}

Then {\tt F[n,j]} computes the Taylor expansion of \( f(t+nk,x+jh) \) about \( (t,x) \). This is useful for compute local truncation errors. For instance, to compute the LTE of a second order centered difference approximation \( f'(x) \approx (f(t+k,x)-f(t-k,x))/2k \) we set \( d=3 \) and use,
\begin{lstlisting}
    FullSimplify[(F[1, 0] - F[-1, 0])/(2 k)]
\end{lstlisting}
This gives that the difference methods is like \( f_t(t,x) + \cO(k^2) \).

\pagebreak
\section{Basic Linear Algebra}

\subsection{Matrix and Vector Norms}
The general definition of a matrix norm is,
\begin{align*}
    \norm{A} = \sup_{u\neq 0} \frac{\norm{Au}}{\norm{u}} = \sup_{\norm{u}=1} \norm{Au}
\end{align*}

Equivalent definition:
\begin{align*}
    \norm{A} = \sup_{u,v\neq 0} \frac{\ip{Au,v}}{\norm{u}\norm{v}} = \sup_{\norm{u}=\norm{v}=1} \ip{Au,v}
\end{align*}
 
If \( A \) is self Hermitian,
\note{double check this}
\begin{align*}
    \norm{A} = \sup_{u\neq 0} \frac{\ip{Au,u}}{\norm{u}^2} = \sup_{\norm{u}=1} \ip{Au,u}
\end{align*}


\subsection{Similar Matrices}
\textit{Definition}: Two matrices \( A \) and \( B \) are similar if \( A = XBX^{-1} \) for some \( X \).

\textit{Why it is useful}: The eigenvalues of similar matrices are the same.

\subsubsection{Diagonalizable}
\textit{Definition}: A matrix \( A \) is diagonalizable if it is similar to a diagonal matrix

\pagebreak
\section{Projectors}
\textit{Definition}: A matrix \( P \) is a projector if \( P^2 = P \)

\textit{Why it is useful}: The eigenvalues of a diagonal matrix are the diagonal entries


\subsection{Orthogonal Projectors}


\pagebreak
\section{Rayleigh Quotients}
\note{Anne seems to like these}

Approximates eigenvalues

Can write every value in range of eigenvalues as rayleigh quotient for real symmetric matrices.


\pagebreak
\section{Classification of Matrices}
Matrices are assumed to be complex and unless specified otherwise.

\subsection{Upper Triangular}
\textit{Definition}: A matrix \( R \) is upper triangular if \( r_{ij} = 0 \) for \( i>j \).
If \( r_{ii} = 0 \) the matrix is called strictly upper triangular. 

\textit{Properties}:
\begin{itemize}[nolistsep]
    \item[\(\Rightarrow\)] Eigenvalues are diagonal entries
    \item[\(\Rightarrow\)] Inverse, product, and sum of upper triangular matrices are upper triangular
    \item[\(\Rightarrow\)] Can solve triangular linear systems in \( \cO(m^2) \) time with back substitution
\end{itemize}


\subsection{Unitary}
\textit{Definition}: A matrix \( U \) is unitary if \( U^*U = UU^* = I  \).

\textit{Properties}:
\begin{itemize}[nolistsep]
    \item[\(\Leftrightarrow\)] Columns are orthonormal and form a basis for \( \CC^n \)
    \item[\(\Rightarrow\)] \( \norm{AU}_2 = \norm{UA}_2 = \norm{A}_2 \)
\end{itemize}


\subsection{Hermitian}
\textit{Definition}: A matrix \( A \) is Hermitian if \( A^* = A \)

\textit{Properties}:
\begin{itemize}[nolistsep]
    \item[\(\Rightarrow\)] Real eigenvalues
    \item[\(\Rightarrow\)] \hyperref[sec:normal]{Normal}
    \item[\(\Rightarrow\)] Unitarily diagonalizable
    \item[\(\Rightarrow\)] \( \norm{A}_2 = \rho(A) \)
\end{itemize}


\subsection{Skew symmetric}
\textit{Definition}: A real matrix \( A \) is skew symmetric if \( A^T = -A \)

\textit{Properties}:
\begin{itemize}[nolistsep]
    \item[\(\Rightarrow\)] pure imaginary eigenvalues
    \item[\(\Rightarrow\)] \( I+A \) is invertible
\end{itemize}


\subsection{Normal}
\label{sec:normal}
\textit{Definition}: A matrix \( A \) is normal if \( A^*A = AA^* \)

\textit{Properties}:
\begin{itemize}[nolistsep]
    \item[\(\Leftrightarrow\)] Unitarily diagonalizable (means eigenvectors are orthogonal)
    \item[\(\Rightarrow\)] Hermitian if all eigenvalues are real
\end{itemize}


\subsection{Positive definite}
\textit{Definition}: A matrix \( A \) is positive definite if all eigenvalues are positive.

\textit{Properties}:
\begin{itemize}[nolistsep]
    \item[\(\Leftrightarrow\)] \( v^*Av > 0 \) for all \( v \)
    \item[\(\Leftrightarrow\)] Has \hyperref[sec:cholesky]{Cholesky} factorization
\end{itemize}


\subsection{???}
Where should I put things like similarity transform, unitarily diagonalizable, etc.

Also things like 

\pagebreak
\section{Matrix Decompositions}
\subsection{SVD}
\begin{align*}
    A = U\Sigma V^*
\end{align*}

\begin{itemize}[nolistsep]
    \item \( U \) unitary
    \item \( \Sigma \) diagonal, with real positive entries in non-increasing order
    \item \( V \) unitary
\end{itemize}

\textit{Existence}:
Always

\textit{Uniqueness}:
\note{double check}
Unique up to complex sign of columns of \( U \) and \( V \)

\textit{Computing}:

\textit{Why it is useful}:
\begin{itemize}[nolistsep]
    \item Gives geometric interpretation for linear transforms on \( \CC^n \)
    \item Rank revealing
    \item Numerical stability of algorithms using SVD
\end{itemize}

\subsubsection{Reduced SVD}
If \( A \) is rank deficient some singular values will be zero. We can drop these singular values and the corresponding singular vectors.


\subsection{(P)LU}

\textit{Existence}:

\textit{Uniqueness}:

\textit{Computing}:
Gaussian Elimination 

\subsubsection{Partial Pivoting}

When is pivoting needed?

\subsubsection{Cholesky}
\label{sec:cholesky}
\textit{Existence}: If \( A \) is Hermitian positive definite

\textit{Uniqueness}: Unique up to sign

\textit{Why it is useful}:
\begin{itemize}[nolistsep]
    \item Save storage space
\end{itemize}

\subsection{QR}


\subsection{Eigen}


% these are less relevant
\subsection{Shur}

\subsection{Jordan Normal}



\section{Direct Methods for Linear Systems}
\subsection{QR}

\subsection{Gaussian Elimination}
\subsection{SVD}



\section{Iterative Methods for Linear Systems}
\subsection{Simple Iteration}

\subsection{Power Iteration}

\subsubsection{Simultaneous Power Iteration (QR)}


\subsection{Conjugate Gradient}

\subsection{GMRES}

\subsection{Other methods}

\section{Solving Least Squares}
The linear least squares problem is,
\begin{align*}
    \min_{x} \norm{b-Ax}_2
\end{align*}

This is solved when \( x \) solve the linear system (called the normal equations),
\begin{align*}
    A^TAx = A^Tb
\end{align*}


\subsection{Derivations of Normal Equations}

\textit{Using Projectors}:


\textit{Using Calculus}:

Note that
\begin{align*}
    \norm{b-Ax} = (b-Ax)^T(b-Ax) = b^Tb + -2b^TAx + x^T(A^TA)x
\end{align*}
Therefore, taking the gradient of \( \norm{b-Ax} \), we know it is minimized when \( 2A^TAx - 2A^Tb = 0 \). 

\note{How do we do derivative of things like x*Ax }

\subsection{Solving Least Squares Numerically}

\pagebreak
\section{Boundary Value Problems}

\subsection{Laplacian}


\pagebreak
\section{Integrators and IVPs}

\subsection{Runge-Kutta Methods}


\subsection{LMMs}


\subsection{Stability}
A method is stable, if when applied to the test equation with \( \lambda < 0 \), the solution doesn't blow up. That is, \( \{ U^n \}_{n=0}^{\infty} \) is bounded.
\note{double check this}

\subsection{Zero Stable}

\subsection{Region of Absolute Stability}

A stable L stable etc

\section{PDEs}
\subsection{Method of Lines}

\subsubsection{Von Neumann Analysis}

\begin{enumerate}[nolistsep]
    \item Replace \( U_j^n \) with \( g(\xi)^n e^{i\xi j \Delta x} \)
    \item Solve for \( g(\xi) \) and compute \( |g(\xi)| \)
    \item Method is stable if and only if for all \( \xi \), \( |g(\xi)| \leq 1+\cO(\Delta x) \)
\end{enumerate}




\end{document}

