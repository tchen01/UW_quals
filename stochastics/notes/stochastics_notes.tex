\documentclass[12pt]{article}
\usepackage[T1]{fontenc}

% Document Details
\newcommand{\CLASS}{Quals Revision Notes}
\newcommand{\assigmentnum}{Stochastics Sequence}


\usepackage[margin = 1.3in, top = 1.25in, bottom = 1.3in]{geometry}
\input{../../TeX_headers/title.tex} % Title Styling
\input{../../TeX_headers/sfftoc.tex} % ToC Styling
\input{../../TeX_headers/styling.tex} % General Styling
\input{../../TeX_headers/section.tex} % Section Styling
\input{../../TeX_headers/code.tex} % Code Display Setup
\input{../../TeX_headers/math.tex} % Math shortcuts
\input{../../TeX_headers/proof.tex} % Proof shortcuts
\input{../../TeX_headers/problem.tex} % Math shortcuts
\setlength{\headheight}{15pt}
\newcommand{\note}[1]{\textcolor{red}{\textbf{Note:} #1}}


\begin{document}
\maketitle

\pagebreak
\section{Introduction}
This document contains personal revision notes for the 2018 Stochastics Qualification Exam in the Applied Mathematics department at the University of Washington. These notes are heavily based on the class notes \cite{lorig} and texts \cite{grimmett} used in the AMATH 561/562/563 courses.

For notational convenience any result which was determined to be worth memorizing was marked as a ``Theorem'' even if the result is not usually classified as a Theorem.

\tableofcontents

\pagebreak

\section{Probability Fundamentals}
\subsection{Events as sets}
\begin{definition}[Sample Space]
The set of all possible outcomes of an experiment is called the sample space and is generally denoted by \( \Omega \).
\end{definition}

\begin{definition}[Event]
    An event is a subset of the sample space.
\end{definition}

\begin{definition}[\( \sigma \)-algebra]
A collection \( \cF \) of subsets of \( \Omega \) is a \( \sigma \)-algebra if it satisfies,
\begin{enumerate}[nolistsep]
    \item contains the empty set: \( \emptyset \in \cF \)
    \item closed under unions: if \( A_i \in \cF \) for \( i=1,2,\ldots \), then \( \cup_i A_i \in \cF \)
    \item closed under compliments: if \( A\in \cF \) then \( A^c \in \cF \)
\end{enumerate}
\end{definition}

\begin{lemma}
If \( \cF \) and \( \cG \) are \( \sigma \)-algebras, then their intersection is also a \( \sigma \)-algebra. More generally, if \( \{\cF_i\}_{i\in I} \) is a family of \( \sigma \)-algebras, then \( \cF = \cap_{i\in I} \cF_i \) is also \( \sigma \)-algebra.
\end{lemma}


\begin{definition}[\( \sigma \)-algebra generated by a set]
Let \( \cG \in 2^\Omega \). The \( \sigma \)-algebra generated by \( \cG \), denoted \( \sigma(\cG) \), is the smallest \( \sigma \)-algebra containing \( \cG \).
\end{definition}

\begin{theorem}
Given \( \cG \in 2^\Omega \), \( \sigma(\cG) \) is the intersection of all \( \sigma \)-algebras containing \( \cG \).
\end{theorem}

\begin{definition}[Measurable Space]
The pair \( (\Omega, \cF) \) where \( \Omega \) is a sample space and \( \cF \) is a \( \sigma \)-algebra of \( \Omega \) is called a measurable space.
\end{definition}

\subsection{Probability}
\begin{definition}[Probability Measure]
A probability measure defined on a measurable space \( (\Sigma, \cF) \) is a function \( \PP: \cF \to [0,1] \) satisfying,
\begin{enumerate}[nolistsep]
    \item \( \PP(\emptyset) = 0 \), \( \PP(\Omega) = 1 \)
    \item If \( A_1, A_2, \ldots \) is a collection of disjoint elemebts of \( \cF \), in that \( A_i\cap A_j = \emptyset \) for all \( i\neq j \), then,
    \begin{align*}
        \PP \left( \bigcup_{i=1}^{\infty}A_i \right) = \sum_{i=1}^{\infty} \PP(A_i)
    \end{align*}
\end{enumerate}
\end{definition}

\begin{definition}[Probability Space]
The triple \( (\Omega, \cF, \PP) \) where \( \Omega \) is a sample space, \( \cF \) is a \( \sigma \)-algebra of \( \Omega \), and \( \PP \) is a probability measure defined on \( (\Omega,\cF) \) is called a measurable space.
\end{definition}

\begin{lemma}
Let \( A_1, A_2, \ldots  \) be a sequence of increasing sets, so that \( A_1 \subseteq A_2 \subseteq \cdots  \), and write \( A \) for their limit,
\begin{align*}
    A = \bigcup_{i=1}^{\infty} A_i = \lim_{i\to\infty}A_i
\end{align*}
Then \( \PP(A) = \lim_{i\to\infty} \PP(A_i) \).
\end{lemma}

\begin{proof}
Write \( A = A_1 \cup (A_2\setminus A_1)\cup (A_3\setminus A_2) \cup \ldots \), the union of disjoint sets. Then,
\begin{align*}
    \PP(A) &= \PP(A_1) + \sum_{i=1}^{\infty} \PP(A_{i+1} \setminus A_{i}) \\
    &= \PP(A_1) + \lim_{n\to\infty} \sum_{i=1}^{n} \left[ \PP(A_{i+1}) - \PP(A_i) \right] \\
    &= \lim_{n\to\infty} \PP(A_n) \tag*{\qed}
\end{align*}
\end{proof}

\begin{definition}[Conditional Probability]
If \( \PP(B) > 0 \)then the conditional probability that \( A \) occurs given that \( B \) occurs is defined  to be,
\begin{align*}
    \PP(A | B) = \dfrac{\PP(A\cap B)}{\PP(B)}
\end{align*}
\end{definition}


\subsection{Infinite Probability Spaces}

\subsubsection{Uniform Lebesgue Measure on (0,1)}
We construct a mathematical model for choosing a number at random on the open interval \( (0,1) \). Therefore, set \( \Omega = (0,1) \). Define,
\begin{align*}
     \PP(\{\omega: \omega\in(a,b)\}) = \mu((a,b)) := b-a, && 0<a\leq b< 1
\end{align*}
where \( \mu \) is the Lebesgue measure.

A natural question is what are all subsets of \( (0,1) \) whose probabilities are determined by our probability measure and the properties of probability measures.

This turns out not to be \( 2^\Omega \), which has some sets whose probabilities are not determined by this measure.

\note{WHY/HOW?}

The correct set is the \( \sigma \)-algebra generated by the open intervals. That is,
\begin{align*}
    \cB((0,1)):= \sigma(\cO), && \cO:=\{A\subseteq (0,1) : A = (a,b), 0\leq a<b\leq 1 \}
\end{align*}

We call \( \cB((0,1)) \) the Borel \( \sigma \)-algebra on \( (0,1) \).

Thus, the appropriate sample space for our experiment is \( (\Omega,\cF,\PP) = ((0,1),\cB(0,1),\mu) \).

\begin{definition}[Borel \( \sigma \)-algebra]
Let \( \Omega \) be some topological space and let \( \cO(\Omega) \) be the set of open sets in \( \Omega \). The Borel \( \sigma \)-algebra on \( \Omega \) is defined as \( \cB(\Omega) := \sigma(\cO(\Omega)) \).
\end{definition}

\subsubsection{Infinite sequence of coin tosses}
We now consider an infinite sequence of coin tosses. Therefore,
\begin{align*}
    \Omega = \text{the set of infinte sequences of Hs and Ts}
\end{align*}

Note that this set is uncountable (Cantor diagonalization argument).

We would like to construct a \( \sigma \)-algebra for this experiment.

First, consider the trivial \( \sigma \)-algebra,
\begin{align*}
    \cF_0 := \{\emptyset,\Omega\}
\end{align*}
Given no information, we can tell if \( \omega \) is in the sets in \( \cF_0 \) because we know \( \omega \in \Omega \) and \( \omega\notin \emptyset \).

Now define,
\begin{align*}
    A_{\text{H}} = \{\omega \in \Omega : \omega_1 = \text{H}\}, &&
    A_{\text{T}} = \{\omega \in \Omega : \omega_1 = \text{T}\}
\end{align*}

Noting that \( A_{\text{H}} = A_{\text{T}}^c \) we see that,
\begin{align*}
    \cF_1 := \{ \emptyset, \Omega, A_{\text{H}}, A_{\text{T}} \}
\end{align*}
is a \( \sigma \)-algebra. Given \( \omega_1 \) it is possible to say whether or not \( \omega \) is in each of the sets in \( \cF_1 \).

Now define,
\begin{align*}
    A_{\text{HH}} = \{\omega \in \Omega : \omega_1 = \text{H}, \omega_2 = \text{H} \}, &&
    A_{\text{TT}} = \{\omega \in \Omega : \omega_1 = \text{T}, \omega_2 = \text{T} \} \\
    A_{\text{TT}} = \{\omega \in \Omega : \omega_1 = \text{T}, \omega_2 = \text{T} \}, &&
    A_{\text{TH}} = \{\omega \in \Omega : \omega_1 = \text{T}, \omega_2 = \text{H} \}
\end{align*}

The \( \sigma \)-algebra generated by these sets and the sets in \( \cF_1 \) is,
\begin{align*}
    \cF_2 = \left\{\begin{array}{l}\emptyset, \Omega, A_{\text{H}}, A_{\text{T}}, A_{\text{HH}}, A_{\text{HT}}, A_{\text{TT}}, A_{\text{TH}} , A_{\text{HH}}^c, A_{\text{HT}}^c, A_{\text{TT}}^c, A_{\text{TH}}^c \\
    A_{\text{HH}}\cup A_{\text{TH}}, A_{\text{HH}}\cup A_{\text{TT}}, A_{\text{HT}}\cup A_{\text{TH}}, A_{\text{HT}}\cup A_{\text{TT}}
    \end{array}  \right\}
\end{align*}

Note that for instance, \( A_{\text{HH}} \cup A_{\text{HT}} = A_{\text{H}} \).

Now, given \( \omega_1 \) and \( \omega_2 \) we can say if \( \omega \) belongs to each of the sets in \( \cF_2 \). Continuing this way we can define \( \sigma \)-algebras \( \cF_n \) for all \( n\in\NN \). Finally, define,
\begin{align*}
    \cF := \sigma(\cF_\infty), && \cF_\infty := \cup_n \cF_n
\end{align*}

Why not take \( \cF = \cF_\infty \)? This would tell us about what happens in finitely many coin tosses, but couldn't have information about sets such as ``sequences for which 40\% of the coin tosses are heads''. However, these are in \( \cF \) as define.

We now construct a probability measure on \( \cF \). Supose there is a probability \( p \) of heads and \( q = 1-p \) of tails. Then,
\begin{align*}
    \PP(\emptyset) = 0, && \PP(\Omega) = 1, && \PP(A_{\text{H}}) = p, && \PP(A_{\text{T}}) = q, \\
    \PP(A_{\text{HH}}) = p^2, && \PP(A_\text{HT}) = pq, && \PP(A_\text{TH}) = pq, && \PP(A_\text{TT}) = q^2, \ldots
\end{align*}

In this way we can define \( \PP(A) \) for each \( A\in\cF_\infty \). It turns out that this uniquely defines \( \PP \) for sets in \( \cF \).

Now define,
\begin{align*}
    A = \left\{ \omega : \lim_{n\to\infty} \dfrac{\text{\# of H in first \( n \) coin tosses}}{n} = \dfrac{1}{2} \right\}
\end{align*}

By the strong law of large numbers we have, \( \PP(A) = 1 \) if \( p=1/2 \) and \( \PP(A) = 0 \) otherwise.

\note{Understand this rigorously}

\begin{definition}[\( \PP \) almost surely]
Let \( (\Omega,\cF,\PP) \) be a probability space. If a set \( A\in\cF \) satisfies \( \PP(A) = 1 \) we say that the event \( A \) occurs \( \PP \) almost surely.
\end{definition}

\subsection{Random Variables and Distributions}
A random variable maps the outcome of an experiment to \( \RR \).

\begin{definition}[Random Variable]
A random variable \( X \) defined on \( (\Omega, \cF) \) is a function \( X: \Omega \to \RR \) with the property that,
\begin{align*}
    \{X \in A\} := \{\omega \in \Omega : X(\omega) \in A \} \in \cF, && \forall A\in \cB(\RR)
\end{align*}
Such a function is said to be \( \cF \)-measurable.
\end{definition}

\begin{definition}[Distribution Function]
The distribution function \( F_X : \RR \to [0,1] \) of a random variable \( X \) defined on a probability space \( (\Omega, \cF, \PP) \) is given by,
\begin{align*}
    F_X(x) = \PP(X\leq x)
\end{align*}
\end{definition}

Note that while a random variable does not depend on \( \PP \), the distribution does.

\begin{lemma}
A distribution function \( F_X \) has the following properties:
\begin{enumerate}[nolistsep]
    \item \( \lim_{x\to-\infty} F(x) = 0 \), \( \lim_{x\to\infty} F(x) = 1 \)
    \item if \( x < y \) then \( F(x) \leq F(y) \) (non-decreasing)
    \item \( F(x+h) \to F(x) \) as \( h\to 0^+ \) (right continuous)
\end{enumerate}
\end{lemma}

\begin{lemma}
Let \( F \) be the distribution function of \( X \). Then,
\begin{enumerate}[nolistsep]
    \item \( \PP(X>x) = 1-F(x)\)
    \item \( \PP(x<X\leq y) = F(y) - F(x) \)
    \item \( \PP(X=x) = F(x) - \lim_{y\to x^-} F(y) \)
\end{enumerate}
\end{lemma}

\begin{definition}[Discrete Random Variable]
A random variable \( X \) is called discrete if it takes values in some countable set \( A = \{x_1,x_2,\ldots  \subset \RR \). We associate a discrete random variable with a probability mass function \( f_X:A\to\RR \) defined by \( f_X(x_i) = \PP(X = x_i) \).
\end{definition}

\begin{definition}[Continuous Random Variable]
A random variable \( X \) is called continuous if its distribution function \( F_X \) can be written as,
\begin{align*}
    F_X(x) = \int_{-\infty}^{x} f_X(u) \d u
\end{align*}
for some \( f_X:\RR\to[0,\infty) \), called the probability density function.
\end{definition}

We can think of the density function \( f_X \) as \( f_X(x)\d x = \PP(X\in\d x) \)

\subsection{Stochastic Processes}
We can intuitively think of a stochastic process as a process which evolves randomly in time.
\begin{definition}[Stochastic Process]
A stochastic Processes is a collection of random variables \( X = (X_t)_{t\in\bT} \), where \( \bT \) is some index set. If the set \( \bT \) is countable we say \( X \) is a discrete time process. If the set \( \bT \) is uncountable we say \( X \) is a continuous time process.

The state space of a stochastic process process \( X \) is the union of the state spaces of \( (X_t)_{t\in\bT} \).
\end{definition}

Two common ways to think of a stochastic process are 1. for any time \( t\in\bT \) we have that \( X_t: \Omega \to \RR \) is a random variable, and 2. for any \( \omega \in \Omega \), \( X(\omega) : \bT\to\RR \) is a function of time.


\subsection{Expectation}
\begin{definition}[Expectation]
Let \( X \) be a random variable defined on \( (\Omega, \cF, \PP) \). The expectation of \( X \), written \( \EE[ X ] \), is defined as,
\begin{align*}
    \EE[ X ] = \int_\Omega X(\omega) \PP(\d \omega)
\end{align*}
\end{definition}


\subsubsection{Lebesgue Integration}

\note{Probably will want to expand this to understand this better. Maybe even move to a new chapter}

\begin{definition}[Indicator Random Variable]
Fix a probability space \( (\Omega, \cF, \PP) \) and let \( A\in \cF \). The Indicator random variable is defined as,
\begin{align*}
    \bOne_A(\omega) = \begin{cases}
        1 & \omega \in A \\
        0 & \omega \notin A
    \end{cases}
\end{align*}
\end{definition}

For disjoint sets \( A \) and \( B \) we have
\begin{align*}
    \bOne_{A\cup B} = \bOne_A + \bOne_B
\end{align*}
and for any two sets \( A \) and \( B \) we have,
\begin{align*}
    \bOne_{A\cap B} = \bOne_A \bOne_B
\end{align*}

\begin{definition}[Simple Random Variable]
A non-negative random variable \( X \) is called simple if it can be written as,
\begin{align*}
    X(\omega) = \sum_{i=1}^{n} x_i \bOne_{A_i}(\omega), && x_i \geq 0, && A_i\in \cF
\end{align*}
where \( (A_i)_{i=1}^{n} \) is a partition of \( \Omega \).
\end{definition}

\begin{definition}[Expectation of Simple Random Variable]
Let \( X \) be a simple random variable. We define the expectation of \( X \) as,
\begin{align*}
    \EE[ X ] := \sum_{i=1}^{n} x_i \PP(A_i)
\end{align*}
\end{definition}

Note that this means \( \EE[ \bOne_A ]= \PP(A) \).


Now consider a random variable \( X \) which is not necessarily simple. Let \( (X_n)_{n\geq 0} \) be ain increasing sequence of simple random variables that converges almost surely to \( X \).

\note{how do we know such sequences exists....}

\note{Is any of this actually relevant??}

\subsubsection{Computing Expectations}

\begin{definition}[Expectation]
The mean value, or expectation, or expected value of the random variable \( X \) with mass function \( f_X \) is defined to be,
\begin{align*}
    \EE[ X ] = \int_\RR xF_X(\d x):= \lim_{\norm{\Pi}\to 0} \sum_i \left( \dfrac{x_{i+1}-x_i}{2} \right) \left( F_X(x_{i+1}) - F_X(x_i) \right)
\end{align*}
where \( \Pi \) is a partition of \( \RR \), meaning,
\begin{align*}
    \Pi = \{x_1, x_2, \ldots, x_n\}, && x_i < x_{i+1}, && \norm{\Pi}:= \sup_i(x_{i+1}-x_i)
\end{align*}

In the discrete or continuous this becomes,
\begin{align*}
    \EE[ X ] = \sum_i x_i f_X(x_i), &&
    \EE[ X ] = \int_{\RR} x f_X(x) \d x
\end{align*}
\end{definition}

\begin{definition}[Expectation of Function]
The mean value, or expectation, or expected value of the function of a random variable \( g(X) \) with mass function \( f_X \) is,
\begin{align*}
    \EE[ g(X)] = \int_\RR g(x) F_X(\d x)
\end{align*}
where \( \Pi \) is a partition of \( \RR \), meaning,

In the discrete or continuous this becomes,
\begin{align*}
    \EE[ X ] = \sum_i g(x_i) f_X(x_i), &&
    \EE[ X ] = \int_{\RR} g(x) f_X(x) \d x
\end{align*}
\end{definition}


\subsection{Change of Measure}
\begin{theorem}
Fix a probability space \( (\Omega,\cF,\PP) \) and let \( Z\geq 0 \) be a random variable with \( \EE[Z] = 1 \). Define \( \tilde{\PP}: \cF \to [0,1] \) by,
\begin{align*}
    \tilde{\PP}(A) := \EE[Z\bOne_A]
\end{align*}
Then \( \tilde{\PP} \) is a probability measure on \( (\Omega,\cF) \). Denote the expectation taken with respect to \( \tilde{\PP} \) by \( \tilde{\EE} \). Then,
\begin{align*}
    \tilde{\EE}[X] = \EE[ZX], && \text{and if } Z>0 \text{ then} &&
    \EE[X] = \EE \left[ \frac{1}{Z}X \right]
\end{align*}
where \( X \) is a random variable defined on \( (\Omega,\cF) \).
\end{theorem}

\begin{definition}[Radon--Nikodyn Derivative]
We call the random variable \( Z \) the Radon--Nikodyn Derivative of \( \tilde{P} \) with respect to \( \PP \).
\end{definition}

\begin{definition}[Absolutely Continuous]
A probability measure \( \PP \) defined on \( (\Omega,\cF) \) is absolutely continuous with respect to another probability measure \( \tilde{\PP} \), written \( \PP \gg \tilde{\PP} \), if,
\begin{align*}
    \PP(A) = 0 && \Longrightarrow && \tilde{\PP}(A) = 0
\end{align*}
\end{definition}

\begin{definition}[Equivalent]
Two probability measures \( \PP \) and \( \tilde{\PP} \) defined on \( (\Omega,\cF) \) are equivalent, written \( \PP\sim\tilde{\PP} \) if,
\begin{align*}
    \PP(A) = 0 && \Longleftrightarrow && \tilde{\PP}(A) = 0
\end{align*}
\end{definition}

Two probability measures are equivalent if and only if the Radon--Nikodyn derivative which relates them is strictly positive.

\section{Information and Conditioning}
\subsection{Information and \( \sigma \)-algebras}

\begin{definition}[Filtration]
A filtration is an increasing sequence of \( \sigma \)-algebras.
\end{definition}

\begin{definition}[\( \sigma \)-algebra generated by \( X \)]
Let \( X \) be a random variable defined on a non-empty space \( \Omega \). The \( \sigma \)-algebra generated by \( X \), denoted \( \sigma(X) \), is the collection of subsets of \( \Omega \) of the form \( \{X\in A\} \) where \( A\in\cB(\RR) \).
\end{definition}

\begin{definition}[\(\cG \)-measurable]
Let \( X \) be a random variable defined on a non-empty space \( \Omega \). Let \( \cG \) be a \( \sigma \)-algbera of subsets of \( \Omega \). If \( \sigma(X) \subset \cG \) then  we say that \( X \) is \( \cG \)-measurable, and write \( X\in \cG \).
\end{definition}

A random variable is \( \cG \)-measurable if and only if the information in \( \cG \) is sufficient to determine the value of \( X \).

\begin{definition}[\( \bF \)-adapted]
Let \( \bF = (\cF_t)_{t\in[0,T]} \) be a filtration and let \( X = (X_t)_{t\in[0,T]} \) be a sequence of random variables. We say this collection of random variables is \( \bF \)-adapted if \( X_t \in \cF_t \) for all \( t\in[0,T] \).
\end{definition}

\subsection{Independence}
\begin{definition}[Independent Sets]
Two events \( A \) and \( B \) are independent if,
\begin{align*}
    \PP(A\cap B) = \PP(A)\PP(B)
\end{align*}
More generally, a family \( \{A_i\}_{i\in I} \) is called independent if,
\begin{align*}
    \PP \left( \bigcap_{i\in J}A_i \right) = \prod_{i\in J} \PP(A_i)
\end{align*}
for all finite subsets \( J \) of \( I \).
\end{definition}

\begin{definition}[Independent \( \sigma \)-algebras]
Let \( \{\cG_i\}_{i=1}^{n} \) be a family of sub-\(\sigma\)-algebras of \( \cF \). We say these \( \sigma \)-algebras are independent if,
\begin{align*}
    \PP\left(\bigcap_{i=1}^{n} A_i\right) = \prod_{i=1}^n\PP(A_i), && \forall A_1\in \cG_1, \ldots, \forall A_n\in\cG_n
\end{align*}
\end{definition}

\begin{definition}[Independent Random Variables]
Let \( \{X_i\}_{i=1}^{n} \) be a family of random variables. We say \( \{X_i\}_{i=1}^{n} \) are independent if \( \{ \sigma(X_i) \}_{i=1}^{n} \) are independent.

We say the random variable \( X \) is independent of the \( \sigma \)-algebra \( \cG \) if \( \sigma(X) \indep \cG \).
\end{definition}

\begin{lemma}
If \( X \) and \( Y \) are independent random variables then,
\begin{align*}
    \EE[ XY ] = \EE[X]\EE[Y]
\end{align*}
\end{lemma}

\begin{definition}[Joint Distribution]
The Joint Distribution \( F_{X,Y} : \RR^2 \to [0,1] \) of two random variables \( X \) and \( Y \) is given by,
\begin{align*}
    F_{X,Y}(x,y) = \PP(X\leq x, Y\leq y)
\end{align*}
\end{definition}

Variables can be jointly discrete or jointly continuous in natural generalization.

\begin{theorem}
Let \( X \) and \( Y \) be random variables. The following are equivalent:
\begin{enumerate}[nolistsep]
    \item \( X\indep Y \)
    \item \( F_{X,Y}(x,y) = F_X(x)F_Y(y) \)
    \item \( f_{X,Y}(x,y) = f_X(x)f_Y(y) \) discrete: for every \( (x,y)\in \RR^2 \), continuous: for almost every \( (x,y)\in \RR^2 \)
    \item \( \EE[\exp(iuX+ivY) = \EE[\exp(iuX)]\EE[\exp(ivY)] \) for all \( (u,v)\in\RR^2 \)
\end{enumerate}
\end{theorem}

\begin{definition}[Variance]
The variance of a random variable \( X \) is defined as,
\begin{align*}
    \VV[ X ] = \EE[(X-\EE[ X ])^2] = \EE[ X ]^2 - (\EE[ X ])^2
\end{align*}
whenever this expectation exists.
\end{definition}

\begin{definition}[Covariance]
The covariance of random variables \( X \) and \( Y \) is defined as,
\begin{align*}
    \cov[ X , Y ] = \EE[(X-\EE[ X ])(Y-\EE[Y])] = \EE[ XY ] - \EE[ X ] \EE[ Y]
\end{align*}
whenever this expectation exists.
\end{definition}

\begin{lemma}
For constant \( a \) and \( b \),
\begin{align*}
    \VV[aX + bY] = a^2 \VV [X] + b^2 \VV [Y] + 2ab\cov[X,Y]
\end{align*}
\end{lemma}


\subsection{Conditional Expectation}

When \( (X,Y) \) are jointly discrete or jointly continuous we have conditional probability mass functions,
\begin{align*}
    \text{discrete: } && f_{X|Y}(x_i,y_j)&:= \PP(X=x_i | Y = y_j) = \dfrac{\PP(X=x_i \cap Y = y_j)}{\PP(Y = y_j)} = \dfrac{f_{X,Y}(x_i,y_j)}{f_Y(y_j)} \\
    \text{continuous: } && f_{X|Y}(x,y) \d x&:= \PP(X \in \d x | Y = y) = \dfrac{\PP(X=\in \d x \cap Y = y)}{\PP(Y = y)} = \dfrac{f_{X,Y}(x,y)}{f_Y(y)}\d x
\end{align*}

From this we can defined the conditional expectation \( \EE[X | Y = y] \) as,
\begin{align*}
    \text{discrete: } && \EE[X|Y=y_j] &:= \sum_{i} x_i f_{X|Y}(x_i,y_j) \\
    \text{continuous: } && \EE[X|Y=y] &:= \int_\RR x f_{X|Y}(x,y)\d x
\end{align*}

Note that \( \EE[X|Y=y] \) is a function of \( y \); there is nothing random about it.

Unfortunately lots of times \( (X,Y) \) will not be jointly discrete or jointly continuous. We need a more general definition of conditional expectation.

\begin{definition}[Conditional Expectation]
Let \( (\Omega, \cF, \PP) \) be a probability space, let \( \cG \) be a sub-\( \sigma \)-algbera of \( \cF \), and let \( X \) be a random variable that is either nonnegative or integrable. The conditional expectation of \( X \) given \( \cG \), denoted \( \EE[X|\cG] \) is any random variable satisfying,
\begin{enumerate}[nolistsep]
    \item Measurability: \( \EE[X | \cG] \in \cG \)
    \item Partial averaging: \( \EE[\bOne_A \EE[X|\cG]] = \EE[\bOne_A X] \) for all \( A\in\cG \). Alternatively, \( \EE[Z \EE[X|\cG]] = \EE[ZX] \) for all \( Z\in\cG \).
\end{enumerate}
When \( \cG = \sigma(Y) \) we will often use the short hand notation \( \EE[X | Y] := \EE[X | \sigma(Y)] \)
\end{definition}

\begin{definition}[Conditional Probability]
The conditional probability of \( A \) given \( \cG \) is,
\begin{align*}
    \PP(A | \cG) = \EE[\bOne_A | \cG]
\end{align*}
\end{definition}

\begin{theorem}
Let \( (\Omega, \cF, \PP) \) be a probability space and let \( \cG \) be a sub-\( \sigma \)-algebra of \( \cF \). Conditional expecations satisfy,
\begin{enumerate}[nolistsep]
    \item Linearity: \( \EE[aX + bY | \cG] = a \EE[X|\cG] + b \EE[Y|\cG] \).
    \item Taking out what is known: if \( X \in \cG \) then \( \EE[XY | \cG] = X \EE[Y | \cG] \).
    \item Iteration Conditioning: if \( \cH \) is a sub-\(\sigma\)-algebra of \( \cG \), then \( \EE[\EE[X|\cG]|\cH] = \EE[X|\cH] \)
    \item Independence: if \( X\indep \cG \), then \( \EE[X|\cG] = \EE[ X ] \)
\end{enumerate}
\end{theorem}

\begin{theorem}[Jensen's Inequality]
    Let \( X \) be a random variable defined on \( (\Omega, \cF,\PP) \) and let \( \cG \) be a sum-\(\sigma\)-algebra of \( \cF \). Suppose \( \varphi:\RR\to\RR \) is a convex function. Then,
    \begin{align*}
        \varphi(\EE[X|\cG]) \leq \EE[\varphi(X) | \cG], && \PP\text{-a.s}
    \end{align*}
\end{theorem}

\begin{definition}[Martingale]
Let \( (\Omega,\cF,\PP) \) be a probability space, let \( T > 0 \), and let \( \bF = (\cF_t)_{t\in[0,T]} \) be a filtration of sum-\(\sigma\)-algebras of \( \cF \). Consider an \( \bF \)-adapted stochastic process \( M = (M_t)_{t\in[0,T]} \). We say that \( M \) is,
\begin{align*}
    \text{a martingale if:} && \EE[M_t | \cF_s] = M_s, && \forall 0\leq s\leq t\leq T, \\
    \text{a sub-martingale if:} && \EE[M_t | \cF_s] \geq M_s, && \forall 0\leq s\leq t\leq T, \\
    \text{a super-martingale if:} && \EE[M_t | \cF_s] \leq M_s, && \forall 0\leq s\leq t\leq T, \\
\end{align*}
\end{definition}

\begin{definition}[Markov]
Let \( (\Omega,\cF,\PP) \) be a probability space, let \( T > 0 \), and let \( \bF = (\cF_t)_{t\in[0,T]} \) be a filtration of sum-\(\sigma\)-algebras of \( \cF \). Consider an \( \bF \)-adapted stochastic process \( X = (X_t)_{t\in[0,T]} \).

We say that \( X \) is Markov if for all \( 0\leq s \leq t \leq T \) and for every nonnegative, Borel-measurable function \( f \), there is another Borel-measurable function \( g \) (which depends on \( s,t \), and \( f \)) such that,
\begin{align*}
    \EE[f(X_t) | \cF_s] = g(X_s).
\end{align*}

Identifying \( g(X_s) = \EE[f(X_t) | X_s] \) we can write the markov property as follows:
\begin{align*}
    \EE[f(X_t) | \cF_s] = \EE[f(X_t) | X_s]
\end{align*}
\note{I don't understand this last bit}
\end{definition}

\begin{definition}[Transition Kernel]
If \( X_t \) is a discrete or continuous random variabel for every \( t \) then we have transition kernel,
\begin{align*}
    \text{discrete: } && P(s,x;t,y) := \PP(X_t = y | X_s = x) \\
    \text{continuous: } && \Gamma(s,x;t,y)\d y := \PP(X_t \in \d y | X_s = x)
\end{align*}
\end{definition}

If you can write down the transition kernel explicitly then you have essentially proved the process is Markov.


\section{Generating and Characteristic Functions}

\subsection{Generating Functions}

\begin{definition}[Probability Generating Function]
Suppose \( X \) is a discrete random variable taking values in \( \ZZ_{\geq 0} \). We defined the probability generating function of \( X \) by,
\begin{align*}
    G_X(s) := \EE[s^X] = \sum_k s^k f_X(k)
\end{align*}
where \( f_X(k) \) is the probability mass function of \( X \).
\end{definition}

Since \( \sum_k f_X(k) = 1 \) we know \( G_X(s) \) exists when \( |s| \leq 1 \). Generally we only care about \( G_X \) and its derivatives at the point \( s=1 \). Power series can be integrated and differentiated term by term within their radius of convergence.

We call \( G_X \) the probaility generating function because the coefficient \( f_X(k) \) of the \( \cO(s^k) \) term is precisely \( \PP(X = k) \). So we can use \( G_X \) to obtain \( f_X \).

Note that,
\begin{align*}
    G'_X(1) = \EE[X], && G''_X(1) = \EE[X(X-1)] = \EE[X^2]-\EE[X]
\end{align*}

Therefore,
\begin{align*}
    \VV[X] = \EE[X^2] - (\EE[X])^2 = G''_X(1) - (G'_X(1))^2 + G'_X(1)
\end{align*}

\begin{theorem}
Suppose \( X\indep Y \). Then \( G_{X+Y}(s) = G_X(s)G_Y(s) \).
\end{theorem}


\begin{theorem}
Suppose \( \{X_i\}_{i\in\NN} \) are iid with common distribution \( X \). Define \( S_n = \sum_{i=1}^{n} X_i \). Then,
\begin{align*}
    G_{S_n}(s) = (G_X(s))^n
\end{align*}
\end{theorem}

\begin{proof}
We have,
\begin{align*}
    G_{S_n}(s) = \EE \left[ s^{X_1+X_2 + \cdots X_n} \right] = \left( \EE[s^X] \right)^n = (G_X(s))^n
\end{align*}
\end{proof}

\begin{theorem}
Suppose \( \{X_i\}_{i\in\NN} \) are iid with common distribution \( X \). Define \( S_n = \sum_{i=1}^{n} X_i \). Let \( N \) be independent of \( \{X_i\}_{i\in\NN} \). Then,
\begin{align*}
    G_{S_n}(s) = G_N(G_X(s))
\end{align*}
\end{theorem}

\begin{proof}
We have,
\begin{align*}
    G_{S_N}(s) &= \EE \left[ s^{X_1+X_2 + \cdots X_N} \right] = \EE \left[ \EE \left[ s^{X_1+X_2 +\cdots+X_N} | N \right] \right]
    \\ & \hspace{2em}= \EE \left[ \left( \EE \left[ s^X \right] \right)^N \right]
    = \EE \left[ (G_X(s))^N \right]
    = G_N(G_X(s))
\end{align*}
\end{proof}

\begin{definition}[Joint Probability Distribution]
Suppose \( X \) and \( Y \) are discrete random variables taking values in \( \ZZ_{\geq 0} \). The joint probaiblity generating function of \( (X,Y) \) is defined as,
\begin{align*}
    G_{X,Y}(s,t) = \EE \left[ s^Xt^Y \right] = \sum_k\sum_m s^kt^m f_{X,Y}(k,m)
\end{align*}
where \( f_{X,Y} \) is the joint probability mass function of \( (X,Y) \).
\end{definition}

\subsection{Branching Processes}
Suppose a population evolves in generations. Let \( Z_n \) be the size of the \( n \)-th generation, and assume \( Z_0 = 1 \). Let \( X_{n,i} \) be the number of children of the \( i \)-th member of the \( n \)-th generation. Then, clearly, the number of individuals in the \( (n+1) \)-th generation is given by,
\begin{align*}
    Z_{n+1} = X_{n,1} + X_{n,2} + \cdots + X_{n,Z_n}
\end{align*}

\begin{theorem}
Assume \( Z_{n+1} \) is as above and that the \( \{X_{n,i}\} \) are iid. Define \( G_n(s) := \EE \left[ s^{Z_n} \right] \) and \( G(s) := \EE \left[ s^X \right] \). Then,
\begin{align*}
    G_{n+m}(s) = G_n(G_m(s)) = G_m(G_n(s)), && \text{and thus} &&
    \underbrace{G_n(s) = G(G(\cdots G(s)\cdots))}_{n\text{-fold iteration}}
\end{align*}
\end{theorem}

\begin{theorem}
Suppose \( \EE[Z_1] = \mu \) and \( \VV[Z_1] = \sigma^2 \). Then,
\begin{align*}
    \EE[Z_n] = \mu^n, &&
    \VV[Z_n] = \begin{cases}
        n\sigma^2 & \mu=1 \\
        \dfrac{\sigma^2(\mu^n-1)\mu^{n-1}}{\mu-1} & \mu\neq 1
    \end{cases}
\end{align*}
\end{theorem}

\subsection{Characteristic Functions}

\begin{definition}[Characteristic Function]
The characteristic function of a random variable \( X \) is the function \( \phi_X:\RR\to\CC \) defined by,
\begin{align*}
    \phi_X(t) := \EE \left[ \exp(i t X) \right]
\end{align*}
\end{definition}

The characteristic function always exists since \( \EE \left[ |\exp(itX)| \right] = 1 \) . Clearly, if \( G_X \) and \( \phi_X \) both exist then,
\begin{align*}
    \varphi_X(t) = G_X(\exp(it))
\end{align*}

\begin{theorem}
Characteristic functions have the following properties:
\begin{enumerate}[nolistsep]
    \item \( \phi_{aX+b}(t) = \exp(ibt)\phi_X(at) \) for all constants \( a,b \)
    \item if \( X \) and \( Y \) are independent, \( \phi_{X+Y}(t) = \phi_X(t)\phi_Y(t) \)
    \item if \( \{X_i\}_{i=1}^{n} \) are independent and \( S_n = \sum_{i=1}^{n} X_i \), then \( \phi_{S_n}(t) = (\phi_X(t))^n \).
\end{enumerate}
\end{theorem}

\begin{theorem}
Let \( \phi_X(t) \) be the characteristic function of a random variable \( X \) with \( \EE[|X|^n] < \infty \). Then,
\begin{align*}
    \phi^{(n)}(0) = i^n \EE[X^n]
\end{align*}
\end{theorem}

\begin{proof}
We have,
\begin{align*}
    \dd[n]{}{t}\phi_X(t) = \EE \left[ \dd[n]{}{t} \exp(itX) \right]
    = i^n \EE \left[ X^n \exp(itX) \right]
\end{align*}
Now take \( t=0 \) to obtain the theorem. \qed
\end{proof}

\begin{theorem}[Inversion]
Suppose \( X \) is a continuous random variable with density \( f_X \). Then,
\begin{align*}
    f_X(x) = \dfrac{1}{2\pi}\int_\RR \exp(-it x)\phi_X(t)\d t
\end{align*}
for all \( x \) where \( f_X \) is differentiable.
\end{theorem}

\begin{definition}[Convergence in Distribution]
We say a sequence of distribution functions \( \{F_n\}_{n\in\NN} \) converges to a distribution function \( F \) if \( \lim_{n\to\infty} F_n(x) = F(x) \) for all points where \( F \) is continuous.
\end{definition}

\begin{theorem}[Continuity]
Let \( \{F_n\}_{n\in\NN} \) be a sequence of distribution functions with corresponding characteristic functions \( \{\phi_n\}_{n\in\NN} \).
\begin{enumerate}[nolistsep]
    \item If \( F_n\to F \), where \( F \) is a distribution function with corresponding characteristic function \( \phi \), then \( \phi_n\to\phi \) pointwise.
    \item Conversely, if \( \phi(t) = \lim_{n\to\infty} \phi_n(t) \) exists and is continuous at time \( t=0 \), then \( \phi \) is the characteristic function of some distribution \( F \), and \( F_n\to F \).
\end{enumerate}

\end{theorem}

\subsection{LLN and CLT}
\begin{definition}[Convergence in Distribution of Random Variables]
We say a sequence \( \{X_n\}_{n\in\NN} \) of random variables converges in distribution to a random variable \( X \), written \( X_n \xrightarrow{\cD} X \), if \( F_{X_n} \to F_X \).
\end{definition}

\begin{theorem}[Law of Large Numbers]
Let \( \{X_n\}_{n\in\NN} \) be a sequence of iid random variables with \( \EE[X_n]=\mu \). Define \( S_n = \frac{1}{n}\sum_{i=1}{n} X_i \). Then \( S_n\xrightarrow{\cD}\mu \)
\end{theorem}

\begin{theorem}[Central Limit Theorem]
Let \( \{X_n\}_{n\in\NN} \) be a sequence of iid random variables with \( \EE[X_n]=\mu \) and \( \VV[X_n] = \sigma^2 \). Define,
\begin{align*}
    S_n = \frac{1}{n}\sum_{i=1}{n} X_i, && U_n = \dfrac{S_n - \mu n}{\sqrt{n\sigma}}
\end{align*}
Then \( U_n \xrightarrow{\cD} Z \), where \( Z\sim\cN(0,1) \).
\end{theorem}

\subsection{Large Deviations Principle}

\begin{definition}[Cumulant Generating Function]
The cumulant generating function \( \Lambda_X \) of \( X \) is defined as,
\begin{align*}
    \Lambda_X(t) = \log(M_X(t)), && M_X(t)  = \EE[ \exp(tX) ]
\end{align*}
\end{definition}

We note that \( \Lambda_X'(0) = mu \) as,
\begin{align*}
    \Lambda_X'(0) = \dfrac{M_X'(0)}{M_X(0)} = \dfrac{\EE[X\exp(tX)]}{\EE[\exp(tX)]} \bigg|_{t=0} = \EE[X] = \mu
\end{align*}

We also note that \( \Lambda_X \) is convex as,
\begin{align*}
    \Lambda_X''(t) = \dfrac{M_X(t)M_X''(t)-(M_X'(t))^2}{M_x^2(t)}
    = \dfrac{\EE[\exp(tX)] \EE[X^2\exp(tX)-\EE[X \exp(tX)]^2}{M_X^2(t)} \geq 0
\end{align*}
where we have used the Cauchy-Schwarz inequality,
\begin{align*}
    \EE[YZ]^2 \leq \EE[Y^2]\EE[Z^2], &&\text{with} && Y = X\exp(tX/2), && Z = \exp(tX/2)
\end{align*}

\begin{definition}[Fenchel-Legendre Transform]
We define the Fenchel-Legendre transform \( \Lambda_X^* \) for \( \Lambda_X \) as,
\begin{align*}
    \Lambda_X^* = \sup_{t\in\RR}\{at-\Lambda_X(t)\}, && a\in\RR
\end{align*}
\end{definition}

\begin{theorem}[Large Deviation Principle]
Let \( \{X_i\} \) be a sequence of iid random variables with common distribution \(
X \). Define \( \mu := \EE[X] \) and suppose the moment generating function \( M_X(t):= \EE[\exp(tX)] \) is finite in some neighborhood of \( t=0 \). Let \( \Gamma_X \) and \( \Gamma_X^* \) be defined as above. Suppose \( a > \mu \) and \( \PP(X>a)>0 \). Then \( \Gamma_X^*(0) > 0 \) and,
\begin{align*}
    \lim_{n\to\infty} \dfrac{1}{n} \log(\PP(S_n > na)) = -\Lambda_X^*(a), && S_n = \sum_{i=1}^{n} X_i
\end{align*}
\end{theorem}

\section{Discrete Time Markov Chains}
A Markov Chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.

\subsection{Overview}
\begin{definition}[Discrete Time Markov Chain]
A discrete time Markov chain \( X = (X_n)_{n\in\NN} \) is a discrete time Markov process with countable state space \( S \).

Equivalently,
\begin{align*}
    \PP(X_n = s |  X_0 = x_0, X_1 = x_1, \ldots, X_{n-1} = x_{n-1}) = \PP(X_n = s | X_{n-1} = x_{n-1})
\end{align*}
for all \( n\geq 1 \) and all \( s, x_1, x_2, \ldots, x_{n-1} \in S \).
\end{definition}

\begin{definition}[Homogeneous DTMC]
A DTMC \( X \) is called homogeneous if,
\begin{align*}
    \PP(X_{n+1} = j | X_n = i) = \PP(X_1 = j | X_0 = i)
\end{align*}
for all \( n, j,i \).
\end{definition}

We assume Markov chains are homogeneous unless otherwise stated.

\begin{definition}[One-Step Transition Matrix]
The one-step transitoin matrix of a DTMC \( X \) is the \( |S|\times |S| \) matrix defined as,
\begin{align*}
    p(i,j) = \PP(X_1 = j | X_0 = i)
\end{align*}
\end{definition}

\begin{theorem}
The transition matrix \( P \) is a stochastic matrix (all entries are non-negative, and rows sum to one).
\end{theorem}

\begin{definition}[\( n \)-Step Transition Matrix]
The \( n \)-step transitoin matrix of a DTMC \( X \) is the \( |S|\times |S| \) matrix defined as,
\begin{align*}
    p_n(i,j) = \PP(X_1 = j | X_0 = i)
\end{align*}
\end{definition}

\begin{theorem}[Chapman-Kolmogorov Equation]
Let \( P \) and \( P_n \) be the one-step and \( n \)-step tranition matrices of a homogeneous DTMC. Then,
\begin{align*}
    P_{m+n} = P_mP_n, && P_n = P^n
\end{align*}
\end{theorem}

\begin{proof}
For any \( i,j\in S \) we have,
\begin{align*}
    p_{m+n}(i,j) &= \PP(X_{m+n} = j | X_0 = i) \\
    &= \sum_k \PP(X_{m+n} = j | X_m = k) \PP(X_m = k | X_0 = i) \\
    &= \sum_k \PP(X_n = j | X_0 = k) \PP(X_m = k | X_0 = i) \\
    &= \sum_k p_n(k,j) p_m(i,k)
\end{align*}
This proves \( P_{m+n} = P_mP_n \). Then clealry,
\begin{align*}
    P_n = P P_{n-1} = P^2 P_{n-2} = \ldots = P^n \tag*{\qed}
\end{align*}
\end{proof}

\begin{lemma}
Let \( X \) be a homogeneous DTMC. Denote the probability mass function of \( X_n \) by,
\begin{align*}
    \mu_n(i) := \PP(X_n = i), && \mu_n = (\mu_n(1), \mu_n(2), \ldots, \mu_n(|S|))
\end{align*}
Then \( \mu_{n+m} = \mu_n P_m \) and thus \( \mu_m = \mu_0 p^m \)
\end{lemma}

\subsection{Classification of States}

\begin{definition}[Persistent/Transient]
Let \( X \) be a DTMC. We say the state \( i \) is persistent or recurrent if,
\begin{align*}
    \PP(X_n = i\text{ for some } n \geq 1 | X_0 = i) = 1
\end{align*}
Otherwise we say the state \( i \) is transient.
\end{definition}

\begin{definition}[First Passage Time]
Let \( X \) be a DTMC. We define the first passage to state \( j \) by,
\begin{align*}
    \tau_j := \inf\{n\geq 1 : X_n = j \}
\end{align*}
and denote the probability mass function of \( \tau_j \) given \( X_0 = i \) by \( f_{ij} \). That is,
\begin{align*}
    f_{ij} = \PP(\tau_j = n | X_0 = i)
\end{align*}
\end{definition}

\begin{theorem}
Define the following generating functions
\begin{align*}
    P_{ij}:= \sum_{=0}^{\infty} s^np_n(i,j),
    && p_0(i,j) = \delta_{i,j},
    && F_{ij}(S) := \sum_{n=0}^{\infty} s^n f_{ij}(n),
    && f_{i,j}(0) = 0
\end{align*}
Then, for any \( |s|<1 \) we have,
\begin{enumerate}[nolistsep]
    \item \( P_{ii}(s) = 1 + F_{ii}(s)P_{ii}(s) \)
    \item \( P_{ij}(s) = F_{ij}(s)P_{jj}(s) \) if \( i\neq j \)
\end{enumerate}
\end{theorem}

\begin{theorem}
\begin{enumerate}
    \item State \( j \) is persistent if \( \sum_n p_n(j,j) = \infty \), and if this holds then \( \sum_n p_n(i,j) = \infty \) for all \( i \) such that \( f_{ij} > 0 \).
    \item State \( j \) is transient if \( \sum_n p_n(j,j) < \infty \), and if this holds then \( \sum_n p_n(i,j) < \infty \) for all \( i \).
\end{enumerate}
\end{theorem}


\begin{theorem}
If \( j \) is transient then \( p_n(i,j) \to 0 \) as \( n\to\infty \) for all \( i \).
\end{theorem}

\begin{definition}[Mean Recurrence Time]
The mean recurrence time of a state \( j \) is defined as,
\begin{align*}
    \hat{\tau}_j = \EE[\tau_j | X_0=j]
\end{align*}
\end{definition}

Clearly \( \overline{\tau}_j = \infty \) if state \( j \) is transient, however \( \overline{\tau}_j \) may be infinite even if \( j \) is recurrent.

\begin{definition}
A recurrent state \( j \) is said to be null if \( \overline{\tau}_j = \infty \), and non-nullor positive if \( \overline{\tau}_j < \infty \).
\end{definition}

\begin{theorem}
A persistent state \( j \) is null if and only if \( p-n(i,j) \to 0 \) as \( n\to\infty \); If this holds then \( p_n(i,j) \to 0  \) as \( n\to\infty \) for all \( i \).
\end{theorem}

\begin{definition}[Period]
The period of a state \( i \) is defined as \( d(i) = \operatorname{gcd}\{n : p_n(i,j) > 0 \}  \). If \( d(i) = 1 \) we say that the state \( i \) is aperiodic.
\end{definition}

\begin{definition}[Ergodic]
A state is called ergodic if it is persistent, non-null, and aperiodic.
\end{definition}


\subsection{Classification of Chains}
\begin{definition}[Communicates]
We say that state \( i \) communicates with state \( j \), written \( i\to j \), if \( p_n(i,j) > 0 \) for some \( n\geq1 \). We say states \( i \) and \( j \) intercommunicate, written \( i\leftrightarrow j \) if \( i\to j \) and \( j\to i \).
\end{definition}

\begin{theorem}
Suppose \( i\leftrightarrow j \). Then,
\begin{enumerate}[nolistsep]
    \item \( i \) is transient if and only if \( j \) is transient.
    \item \( i \) is null persistent if and only if \( j \) is null persistent.
    \item \( i \) and \( j \) have the same period (\( d(i) = d(j) \)).
\end{enumerate}
\end{theorem}

\begin{definition}[Closed]
A set \( C \) of states is called closed if \( p(i,j) = 0 \) for all \( i\in C \) and \( j\notin C \).
\end{definition}

\begin{definition}[Irreducible]
A set \( C \) of states is called irreducible if \( i\leftrightarrow j \) for all \( i,j\in C  \).
\end{definition}

\begin{theorem}[Markov Chain Decomposition]
The state space \( S \) of a Markov chain can be uniquely partitioned as,
\begin{align*}
    S = T \cup C_1 \cup C_2\cup \ldots
\end{align*}
where \( T \) is the set of transient states, and \( C_1, C_2, \ldots \) are closed sets of persistent states.
\end{theorem}

\begin{proof}
Let \( C_1, C_2, \ldots \) be the persistent equivalence classes of \( \leftrightarrow \). We need only show that each \( C_r \) us closed. Suppose on the contrary that there exists \( i\in C_r \) and \( j\notin C_r \) with \( p(i,j) > 0 \). Now, \( j\not\to i \) so,
\begin{align*}
    \PP(X_n\neq i \text{ for all }n\geq1 | X_0 = 1)
    \geq \PP(X_1 = j | X_0 = i)
    = p(i,j) > 0
\end{align*}
This contradicts the assumption that \( i \) is persistent. \qed
\end{proof}

\begin{lemma}
If \( S \) is finite, then at least one state is persistent and all persistent states are non-null.
\end{lemma}


\subsection{Stationary Distributions and the Limit Theorem}

\begin{definition}[Stationary Distribution]
Let \( X \) be a Markov chain with one-step transition matrix \( P \). We say that a row vector \( \pi = (\pi(1),\pi(2), \ldots, \pi(|S|)) \) is a stationary or invariant distribution if,
\begin{align*}
    \pi P = \pi,  && \sum_i \pi(i) = 1, && \text{and} && \pi(i) \geq 0
\end{align*}
\end{definition}

\begin{theorem}
An irreducible chain \( X \) has a stationary distribution \( \pi \) if and only if all states are non-null persistent. In this case \( \pi \) is unique and given by,
\begin{align*}
    \pi(i) = 1/\overline{\tau}_i
\end{align*}
\end{theorem}

\subsection{Reversibility}

\begin{definition}[Time Reversal]
Let \( X = (X_n)_{0\leq n\leq N} \) be a Markov chain. The time reversal of \( X \) is the process \( Y = (X_{N-n})_{0\leq n\leq N} \).
\end{definition}

\begin{theorem}
Let \( X = (X_n)_{0\leq n\leq N} \) be an irreducible Markov chain with one step transition matrix \( P \) and invariant distribution \( \pi \). Suppose \( X_0 = \pi \) (so that \( \mu_n = \pi \) for every \( n \)). Then \( Y \), the time reversal of \( X \), is a Markov chain and its one-step transition matrix \( Q = (q(i,j))_{ij} \) is given by,
\begin{align*}
    q(i,j) = \dfrac{\pi(i)}{\pi(j)} p(j,i)
\end{align*}
\end{theorem}

\begin{definition}[Reversible Markov Chain]
Let \( X = (X_n)_{n\geq 0} \) be an irreducible Markov chain with one-step transition matrix \( P \) and invariant distribution \( \pi \). Suppose \( X_0 = \pi \) (so that \( \mu_n = \pi \) for every \( n \)). Let \( Y \) be the time reversal of \( X \) and denote by \( Q \) the one-step transition matrix of \( Y \). We say that \( X \) is reversible if \( P = Q \), or equivalently if,
\begin{align*}
    \pi(i) p(i,j) = \pi(j) p(j,i)
\end{align*}
\end{definition}

\begin{definition}[Detailed Balance]
Let \( P \) be an \( |S|\times |S| \) one-step transition matrix. We say that a distribution \( \lambda = (\lambda(1), \lambda(2),\ldots, \lambda(|S|)) \) is in detailed balance with \( P \) if for all \( i,j\in S \),
\begin{align*}
    \lambda(i) p(i,j) = \lambda(j) p(j,i)
\end{align*}
\end{definition}

\begin{theorem}
Let \( P \) be the one-step transition matrix of an irreducible Markov chain \( X \). Suppose there exists a distribution \( \lambda \) such that \( \lambda(i) p(i,j) = \lambda(j) p(j,i) \) for all \( i,j\in S \). Then \( \lambda \) is a stationary distribution for \( X \).
\end{theorem}

\subsection{Chains with Finitely Many States}
\begin{theorem}[Perron-Frobenius]
If \( P \) is a one-step transition matrix of a finite irreducible Markov chain \( X \) with period \( d \), then,
\begin{enumerate}[nolistsep]
    \item \( \lambda_1 = 1 \) is an eigenvalue of \( P \)
    \item The \( d \) complex roots of unity,
    \begin{align*}
        \lambda_n = \omega^{n-1}, && \omega = e^{2\pi i/d}, && n=1,2,\ldots, d
    \end{align*}
    are eigenvalues of \( P \),
    \item The remaining eigenvalues \( \lambda_{d+1}, \lambda_{d+2}, \ldots, \lambda_{|S|} \) all have modulus less than one.
\end{enumerate}
\end{theorem}


\section{Continuous Time Markov Chains}
\subsection{The Poisson Process}

\begin{definition}[Counting Process]
A counting process is a stochastic process \( N = (N_t)_{t\geq 0} \) taking values in \( S = \{0,1,2,\ldots\} \) such that:
\begin{enumerate}[nolistsep]
    \item \( N_0 = 0 \)
    \item if \( s<t \) then \( N_s\leq N_t \)
\end{enumerate}
\end{definition}

A counting process counts the number of times an event occurs propr to a given time.

\begin{definition}[Poisson Process]
A Poisson process with intensity \( \lambda \) is a stochastic process \( N = (N_t)_{t\geq 0} \) taking values in \( S = \{0,1,2,\ldots\} \) such that:
\begin{enumerate}[nolistsep]
    \item \( N_0 = 0 \)
    \item if \( s<t \) then \( N_s\leq N_t \)
    \item if \( s<t \) then \( (N_t-N_s) \indep N_s \)
    \item As \( s\to 0^+ \),
    \begin{align*}
        \PP(N_{t+s} = n+m | N_t = m) = \begin{cases}
            \lambda s + \cO(s^2) & m=1\\
            \cO(s^2) & m\geq 1 \\
            1-\lambda s + \cO(s^2) & m=0
        \end{cases}
    \end{align*}
\end{enumerate}
\end{definition}

\begin{lemma}
A Poisson process \( N \) is Markov.
\end{lemma}

\begin{proof}
Let \( \cF_t := \sigma(N_s: 0\leq s\leq t) \). Then,
\begin{align*}
    \EE[g(N_t)|\cF_s] = \EE[g(N_t-N_s+N_s)|N_s] = \sum_{n=0}^\infty g(n+N_s)f_{N_t-N_s}(n)
\end{align*}
\end{proof}

\begin{theorem}
Let \( N = (N_t)_{t\geq 0} \) be a Poisson process with parameter \( \lambda \). Then, for all \( t\geq 0 \) we have \( N_t\sim\operatorname{Poi}(\lambda t) \). That is,
\begin{align*}
    p_t(j) := \PP(N_t = j) = \dfrac{(\lambda t)^j}{j!}\exp(-\lambda t)
\end{align*}
\end{theorem}

\begin{proof}
    p54 of pdf understand this.
\end{proof}

\begin{definition}[Arrival Time]
Let \( N = (N_t)_{t\geq 0} \) be a counting process. We define \( S_n \), the \( n \)-th arrival time by,
\begin{align*}
    S_0 = 0, && S_n = \inf\{t\geq 0 : N_t = n\}, && n\geq 1
\end{align*}
with \( \inf \emptyset = \infty \).
\end{definition}

\begin{definition}[Inter-Arrival Time]
Let \( N = (N_t)_{t\geq 0} \) be a counting process. We define \( \tau_n \), the \( n \)-th inter-arrival time by,
\begin{align*}
    \tau_n = S_n - S_{n-1}, && n\geq 1
\end{align*}
\end{definition}

\begin{theorem}
Suppose the inter-arrival times \( \tau_i \) of a counting process \( N = (N_t)_{t\geq 0} \) are iid and exponentially distributed with parameter \( \lambda \). Then \( N \) is a Poisson process with parameter \( \lambda \).
\end{theorem}

\begin{proof}
    also may be worth understanding
\end{proof}

\subsection{Overview of Continuous Time Markov Chains}

\begin{definition}[Continuous Time Markov Chain (CTMC)]
A continuous time Markov chain \( X = (X_t)_{t\geq 0} \) is a Markov process witha  countable state space \( S \).
\end{definition}

\begin{definition}[Homogeneous]
A CTMC \( X \) is called homogeneous if,
\begin{align*}
    \PP(X_{t+s} = j | X_s = i) = \PP(X_t = j | X_0 = i) =: p_t(i,j), && \forall t,s > 0, \forall i,j\in S
\end{align*}
\end{definition}

\begin{definition}[Transition Semigroup]
Let \( P_t \) be the \( |S|\times|S| \) matrix \( P_t = (p_t(i,j)) \). We call the collection of matrices \( (P_t)_{t\geq 0} \) the transition semigroup.
\end{definition}

\begin{theorem}
The transition semigroup satisfies the follow:
\begin{enumerate}[nolistsep]
    \item \( P_0 = I \)
    \item \( \sum_j  p_t(i,j) = 1 \)
    \item \( P_tP_s = P_{t+s} \)
\end{enumerate}
\end{theorem}

\begin{definition}[Generator of CTMC]
A CTMC with generator \( G = (g(i,j)) \) is a stochastic process satisfying,
\begin{enumerate}[nolistsep]
    \item if \( s<t \) then \( (X_t - X_s) \indep X_s \)
    \item As \( s\to 0 \) we have \( \PP(X_{t+s} = j | X_t = i) = \delta_{i,j} + g(i,j) s + \cO(s^2) \)
\end{enumerate}

More compactly,
\begin{align*}
    G = \lim_{s\to 0^+} \dfrac{1}{s} \left( P_s - I \right)
\end{align*}
\end{definition}

\begin{theorem}[Kolmogorov Forward and Backward Equations]
Let \( X = (X_t)_{t\geq 0} \) be a CTMC with generator \( G \). The transition semigroup of \( X \) satisfies the following ODEs:
\begin{align*}
    \text{Kolmogorov Forward Equation: } && \dd{}{t} P_t &= P_t G, \\
    \text{Kolmogorov Backward Equation: } && \dd{}{t} P_t &= G P_t
\end{align*}
\end{theorem}

\begin{theorem}
Let \( X = (X_t)_{t\geq 0} \) be a CTMC with generator \( G \) and let \( P_t \) be the semigroup generated by \( G \). Then the invariant distribution \( \pi \) satisfies,
\begin{align*}
    \pi = \pi P_t && \Longleftrightarrow && \pi G = 0
\end{align*}
\end{theorem}


\section{Brownian Motion}

\subsection{Scaled Random Walk}
Define,
\begin{align*}
    M_0 = 0, &&
    M_k = \sum_{i=0}^{k} X_i, &&
    X_i = \begin{cases}
        +1 & \omega_i = \text{H} \\
        -1 & \omega_i = \text{T}
    \end{cases}
\end{align*}

\begin{definition}[Brownian Motion]
Let \( (\Omega, \cF, \PP) \) be a probability space. A Brownian motion is a stochastic process \( W = (W_t)_{t\geq 0} \) satisfying,
\begin{enumerate}[nolistsep]
    \item \( W_0 = 0 \).
    \item if \( 0\leq r < s< t< u<\infty \) then \( (W_u-W_t) \indep (W_s-W_r) \).
    \item if \( 0\leq r < s <\infty \) then \( (W_s-W_r)\sim \cN(0,r-s) \).
    \item the map \( t\to W_t(\omega) \) is continuous for every \( \omega \).
\end{enumerate}
\end{definition}

\begin{definition}[Filtration for a Brownian Motion]
Let \( (\Omega, \cF, \PP) \) be a probability space on which a Brownian motion \( W = (W_t)_{t\geq 0} \) is defined. A filtration for the Brownian motion \( W \) is a collection of \( \sigma \)-algebras satisfying,
\begin{enumerate}[nolistsep]
    \item information accumulates: if \( 0\leq s<t<\infty \) then \( \cF_s \subseteq \cF_t \).
    \item adaptivity: for all \( t\geq 0 \), \( W_t\in\cF_t \).
    \item independence of future increments: if \( 0\leq t < u <\infty \), \( W_u-W_t)\indep \cF_t \)
\end{enumerate}
\end{definition}

\begin{definition}[Natural Filtration for Brownian Motion]
The natural filtration for a Brownian motion \( W = (W_t)_{t\geq 0} \) is defined as, \( \cF_t = \sigma(W_u, 0\leq u \leq t) \).
\end{definition}

In theory a filtration could contain more information that by observing the Brownian motion, however information in the filtration is not allowed to destroy the independence of future increments.

If \( \cF_t \) is a filtration for \( W \), then \( W \) is a martingale with respect to this filtration as,
\begin{align*}
    \EE[ W_T | \cF_t] = \EE[W_T - W_t  + W_t) | \cF_t]
    = \EE[W_T - W_t | \cF_t] + \EE[ W_t | \cF_t]
    = 0 + W_t = W_t
\end{align*}

\begin{definition}[Quadratic Variation]
Let \( f : [0,T]\to\RR \). The quadratic variation of \( f \) up to time \( T \) is defined as,
\begin{align*}
    [f,f]_T := \lim_{\norm{\Pi}\to 0} \sum_{j=0}^{n-1} \left[ f(t_{j+1})-f(t_j) \right]^2
\end{align*}
where \( \Pi = \{t_i : 0=t_0<t_1 < \cdots < t_n = T\} \) and \( \norm{\Pi} = \max_i (t_{i+1} - t_i) \).
\end{definition}

\begin{lemma}
Suppose \( f:[0,T]\to \RR \) has a continuous first derivative. Then \( [f,f]_T = 0 \).
\end{lemma}

\begin{theorem}
Let \( W = (W_t)_{t\geq 0} \) be a Brownian motion. Then for all \( T\geq0 \), \( [W,W]_T = T \) almost surely.
\end{theorem}

\begin{definition}[Covariation]
Let \( f,g : [0,T]\to\RR \). The covariation of \( f \) and \( g \) up to time \( T \) is defined as,
\begin{align*}
    [f,g]_T := \lim_{\norm{\Pi}\to 0} \sum_{j=0}^{n-1} \left[ f(t_{j+1})-f(t_j) \right]\left[ g(t_{j+1})-g(t_j) \right]
\end{align*}
where \( \Pi = \{t_i : 0=t_0<t_1 < \cdots < t_n = T\} \) and \( \norm{\Pi} = \max_i (t_{i+1} - t_i) \).
\end{definition}

\begin{theorem}
Let \( W = (W_t)_{t\geq 0} \) be a Brownian motion. Then for all \( T\geq0 \), \( [W,\operatorname{Id}]_T = 0 \) almost surely.
\end{theorem}

This gives us the Heuristics,
\begin{align*}
    \d W_t \d W_t = \d t, && \d W_t \d t = 0, && \d t\d t = 0
\end{align*}

\subsection{Markov Property of Brownian Motion}
\begin{theorem}
Let \( W = (W_t)_{t\geq 0} \) be a Brownian motion and let \( (\cF_t)_{t\geq 0} \) be a filtration for \( W \). Then \( W \) is a Markov process.
\end{theorem}

\begin{proof}
Let \( 0\leq t\leq T \) and let \( f \) be a non-negative Borel measurable function. Then, since \( W_t\in\cF_t \) and \( (W_T-W_t) \indep \cF_t \)
\begin{align*}
    \EE[f(W_T) | \cF_t] = \EE[f(W_T - W_t+W_t) | \cF_t]
    = \int_\RR f(y+ W_t) \Gamma(t,W_t;T,y) \d y
\end{align*}
where \( \Gamma(t,W_t;T,\cdot) \) is the density of a normal random variable with mean \( W_t \) and variance \( T-t \).
Thus, setting \( g(w) =  \int_\RR f(y+ w) \Gamma(t,w;T,y) \d y \) we have,
\begin{align*}
    \EE[f(W_t) | \cF_t] = g(W_t) \tag*{\qed}
\end{align*}
\end{proof}

\subsection{First Hitting Time of Brownian Motion}

\begin{theorem}
Let \( W = (W_t)_{t\geq 0} \) be a Brownian motion and let \( (\cF_t)_{t\geq 0} \) be a filtration for \( W \). Define a process \( Z = (Z_t)_{t\geq 0} \) by,
\begin{align*}
    Z_t = \exp\left(-\frac{1}{2}\sigma^2 t + \sigma W_t\right)
\end{align*}
Then \( Z \) is a martingale with respect to \( (\cF_t)_{t\geq 0} \).
\end{theorem}

\begin{proof}
Let \( T\geq t \). Then,
\begin{align*}
    \EE[Z_T | \cF_t]
    &= \EE \left[ \exp \left( -\frac{1}{2}\sigma^2 T + \sigma W_T \right) \bigg| \cF_t \right]
    \\&= \exp\left( -\frac{1}{2}\sigma^2 T + \sigma W_t \right) \EE \left[ \exp \left(\sigma (W_T -W_t)\right) \big| \cF_t \right]
    \\&= \exp\left( -\frac{1}{2}\sigma^2 T + \sigma W_t \right) \exp \left( \frac{1}{2}\sigma^2(T-t) \right)
    \\&= Z_t
\end{align*}
\end{proof}

\note{good to know \( \EE[\exp(X)] = \exp(\mu + \sigma^2/2) \) when \( X\sim \cN(0,\sigma^2) \)}

\begin{definition}[First Hitting Time]
The first hitting time of a Brownian motion \( W \) to level \( m \) is defined as,
\begin{align*}
    \tau_m := \inf\{t \geq 0 : W_t = m\}
\end{align*}
\end{definition}

\begin{theorem}
For any \( m\in\RR \) and \( \alpha > 0 \) we have,
\begin{align*}
    \EE[\exp(-\alpha \tau_m)] = \exp(-|m| \sqrt{2\alpha})
\end{align*}
\end{theorem}

\subsection{Reflection Principle}
For every trajectory of a Brownian motion that hits level \( m \) prior to time \( t \) and finishes at level \( W_t = w \leq m \), there is an equally likely path that finishes at a level \( W_t = 2m-w  \). Then,
\begin{align*}
    \PP(\tau_m \leq t, W_t\leq w) = \PP(W_t \geq 2m-w), && m>0, && w\leq m
\end{align*}

\begin{theorem}
For all \( m\neq0 \), the first hitting time of a brownian motion has density \( f_{\tau_m} \) give by,
\begin{align*}
    f_{\tau_m}(t) = \bOne_{t\geq 0}\frac{|m|}{t\sqrt{2\pi t}} \exp(-m^2/2t)
\end{align*}
\end{theorem}

\begin{theorem}
For any \( t > 0 \) the joint density of a Brownian motion \( W_t \) and its running maximum \( \overline{W}_t \) is,
\begin{align*}
    f_{W_t,\overline{W}_t}(w,m) = \dfrac{2(2m-w)}{t\sqrt{2\pi t}}\exp(-(2m-w)^2/2t), && m>0, && w\leq m
\end{align*}
\end{theorem}

\section{Stochastic Calculus}

\subsection{It\^o Integrals}

\begin{definition}[Simple Process]
The process \( \Delta = (\Delta_t)_{t\geq 0} \) si called simple if it is of the form,
\begin{align*}
    \Delta_t = \sum_{j=0}^{n-1}\Delta_{t_j} \bOne_{t_j\leq t \leq t_{j+1}}, && 0 = t_0 <t_1<\cdots <t_n = T, && \Delta_{t_j}\in\cF_{t_j}
\end{align*}
\end{definition}

\begin{definition}[Integral of Simple Process]
Given a simple process \( \Delta \) define,
\begin{align*}
    I_T = \int_0^T \Delta_t \d W_t := \sum_{j=0}^{n-1} \Delta_{t_j} (W_{t_{j+1}} - W_{t_j})
\end{align*}
\end{definition}

\begin{theorem}
The process \( I = (I_t)_{t\geq 0} \) is a martingale with respect to \( \cF \).
\end{theorem}

\begin{theorem}
The process \( I = (I_t)_{t\geq 0} \) satisfies,
\begin{align*}
    \VV[I_T] = \EE[I_T^2] = \EE \left[ \int_0^T \Delta_t^2 \d t \right]
\end{align*}
\end{theorem}

\begin{theorem}
The process \( I = (I_t)_{t\geq 0} \) satisfies,
\begin{align*}
    [I,I]_T^2 = \EE \left[ \int_0^T \Delta_t^2 \d t \right]
\end{align*}
\end{theorem}

\subsubsection{It\^o Integrals for General Integrands}

We generalize our integral by defining the integral of a \( \bF \)-adapted process \( \Delta  \) which is square integrable as the limit of the integrals of a sequence converging to \( \Delta \) in the sense that,
\begin{align*}
    \lim_{n\to\infty} \EE \left[ \int_0^T \left( \Delta_t - \Delta_t^{(n)} \right)^2 \d t \right] = 0
\end{align*}

\note{How have we defined the integral of the square???}

\begin{theorem}
Let \( W \) be a Brownian motion and let \( \bF = (\cF_t)_{t\geq 0} \) be a filtration for \( W \). Let \( \Delta = (\Delta_t)_{0\leq t\leq T} \) be adapted to the filtration \( \bF \). Let \( I = (I_t)_{0\leq t\leq T} \) be give by \( I_t = \int_0^t \Delta_s\d s \). Then the process \( I \) has the following properties,
\begin{enumerate}[nolistsep]
    \item The sample paths of \( I \) are continuous.
    \item The process \( I \) is \( \bF \)-adapted. That is, \( I_t\in\cF_t \) for all \( t \).
    \item If \( \Gamma = (\Gamma_t)_{0\leq t\leq T} \) satisfies the same conditions as \( \Delta \), then,
    \begin{align*}
        \int_0^T(a\Delta_t + b\Gamma_t)\d W_t = a\int_0^T \Delta_t \d W_t + b\int_0^T \Gamma_t \d W_t
    \end{align*}
    \item The process \( I \) is a martingale with respect to \( \bF \).
    \item The It\^o isometry \( \EE[I_t^2]] = \EE \left[ \int_0^T \Delta_t^2 \d t \right] \) holds.
    \item The quadratic variation of \( I \) is give by \( [I,I]_T = \int_0^T \Delta_t^2 \d t \).
\end{enumerate}
\end{theorem}

\subsection{It\^o Formula}
Since \( W_t \) is not differentiable we no longer have the chain rule,
\begin{align*}
    \d f(g(t)) = f'(g(t)) g'(t)\d t
\end{align*}

\begin{theorem}
Let \( W = (W_t)_{t\geq 0} \) be a Brownian motion and suppose \( f:\RR\to\RR \) satisfies \( f\in C^2(\RR) \). Then, for any \( T \geq 0 \),
\begin{align*}
    f(W_T) - f(W_0) = \int_0^T f'(W_t)\d W_t + \frac{1}{2} \int_0^T f''(W_t)\d t
\end{align*}
\end{theorem}

We can use this to compute \( \int_0^T W_t \d t \). By the above we have,
\begin{align*}
    W_T^2 - W_0^2 = \int_0^T 2 W_t\d t + \int_0^T \d t
\end{align*}
where we have chosen \( f(x) = x^2 \). Thus,
\begin{align*}
    \int_0^T W_t \d t = \frac{1}{2}W_T^2 - \frac{1}{2}T
\end{align*}

\begin{definition}[It\^o Process]
Let \( W = (W_t)_{t\geq 0} \) be a Brownian motion and let \( \bF = (\cF_t)_{t\geq 0} \) be a filtration for \( W \). An It\^o process is any process \( X = (X_t)_{t\geq 0} \) for the form,
\begin{align*}
    X_t = X_0 + \int_0^t \Theta_s\d s + \int_0^t \Delta_s \d W_s
\end{align*}
where \( \Theta = (\Theta_t)_{t\geq 0} \) and \( \Delta = (\Delta_t)_{t\geq 0} \) are adapted tot he filtration \( \bF \) and satisfy,
\begin{align*}
    \int_0^T |\Theta_t|\d t < \infty, && \int_0^T \Delta_t^2\d t < \infty, && \forall T \geq 0
\end{align*}
and \( X_0 \) is not random.
\end{definition}

\begin{definition}[Differential Form of It\^o Process]
We can write a process in its differential form,
\begin{align*}
    \d X_t \Theta_t \d t + \Delta_t \d W_t
\end{align*}
This expression literally means that \( X \) satisifes the itnegral form.
\end{definition}

\begin{lemma}
The quadratic variation \( [X,X]_T \) of an It\^o process is give by,
\begin{align*}
    [X,X]_T = \int_0^T \Delta_t^2 \d t
\end{align*}
\end{lemma}

\begin{definition}[Integral with respect to It\^o Process]
Let \( X = (X_t)_{t\geq 0} \) be an It\^o process and let \( \Gamma = (\Gamma_t)_{t\geq0} \) be adapted tot he filtration of the Brownian motion \( \bF = (\cF_t)_{t\geq 0} \). We define,
\begin{align*}
    \int_0^T \Gamma_t \d X_t := \int_0^T \Gamma_t \Theta_t \d t + \int_0^T \Gamma_t \Delta_t \d W_t
\end{align*}
where we assume,
\begin{align*}
    \int_0^T |\Gamma_t\Theta_t| \d t < \infty,
    && \int_0^T (\Gamma_t \Delta_t)^2 < \infty,
    && \forall T\geq 0
\end{align*}
\end{definition}

\begin{theorem}[It\^o Formula]
Let \( X = (X_t)_{t\geq 0} \) be an It\^o process and suppose \( f:\RR\to\RR \) satisfies \( f\in C^2(\RR) \). Then, for any \( T\geq 0 \),
\begin{align*}
    f(X_T) - f(X_0) = \int_0^T f'(X_t)\d X_t + \frac{1}{2}\int_0^T f''(X_t)\d[X,X]_t
\end{align*}
\end{theorem}

\begin{lemma}
Let \( W = (W_t)_{t\geq 0} \) be a Brownian motion. Suppose \( g:\RR_+\to\RR_+ \) is a deterministic function. Then,
\begin{align*}
    I_T:=\int_0^T g(t)\d W_t \sim \cN(0,v(T)), && v(T) = \int_0^T g^2(t)\d t
\end{align*}
\end{lemma}

\subsection{Multivaraite Stochastic Calculus}
\begin{definition}[Multidimensional Brownian Motion]
A \( d \)-dimensional Brownian motion is a process,
\( W = (W_t^1, W_t^2, \ldots, W_t^d)_{t\geq 0} \) satisfying,
\begin{enumerate}[nolistsep]
    \item Each \( W_t^i \) is a one dimensional Brownian motion
    \item The processes \( \{W_t^i\}_{i=1}^d \) are independent
\end{enumerate}
\end{definition}

\begin{definition}[Filtration for Multidimensional Brownian Motion]
A filtration for \( W \) is a collection of \( \sigma \)-algebras \( \bF = (\cF_t)_{t\geq 0} \) such that,
\begin{enumerate}[nolistsep]
    \item information accumulates: \( \cF_s \subseteq \cF_t \) for all \( 0\leq s < t \)
    \item adaptivity: \( W\in\cF_t \) for all \( t\geq 0 \)
    \item independent increments: \( (W_t-W_s)\indep \cF_t \)  for all \( 0\leq s < t \)
\end{enumerate}
\end{definition}

\begin{theorem}
Let \( W = (W_t^1, W_t^2, \ldots, W_t^d)_{t\geq 0} \) be a \( d \)-dimensional Brownian motion. The covariation of independent components of \( W \) is zero. That is, \( [W^i, W^j]_T = 0 \) for all \( T\geq 0 \).
\end{theorem}

\begin{theorem}
Let \( W = (W_t^1, W_t^2, \ldots, W_t^d)_{t\geq 0} \) be a \( d \)-dimensional Brownian motion and let \( X^i = (X_t^i)_{t\geq 0} \), \( i=1,2,\ldots, n \) be the It\^o procsses given by,
\begin{align*}
    \d X_t^i = \Theta_t^i \d t + \sum_{j=1}^{d} \sigma_t^{ij} \d W_t^j, && i=1,2,\ldots, n
\end{align*}
Then,
\begin{align*}
    \d[X^i,X^j] = \sum_{k=1}^{d} \sigma_t^{ik} \sigma_t^{jk} \d t
\end{align*}
\end{theorem}

\begin{theorem}[Multidimensional It\^o formula]
Let \( X = (X_t^1, X_t^2, \ldots, X_t^n)_{t\geq 0} \) be an \( n \)-dimensional It\^o process and suppose \( f:\RR^n\to \RR \) satisfies \( f\in C^2(\RR^n) \). Then, for any \( T\geq 0 \),
\begin{align*}
    \d f(X_t) = \sum_{i=1}^{n} \pp{f(X_t)}{x_i} \d X_t^i  + \frac{1}{2} \sum_{i=1}^{n} \sum_{j-1}^{n} \pp{{^2}f(X_t)}{x_i \partial x_j} \d[X^i,X^j]_t
\end{align*}
\end{theorem}

\begin{lemma}[Product Rule]
Let \( X \) and \( Y \) be two one-dimensional It\^o processes. Then,
\begin{align*}
    \d(X_tY_t) = Y_t\d X_t + X_t \d Y_t + \d [X,Y]_t
\end{align*}
\end{lemma}

\begin{theorem}[L\'evy Characterization of Brownian Motion]
Let \( M = (M_t)_{t\geq0} \) be a martingale with respect to a filtration \( \bF = (\cF_t)_{t\geq0} \). Suppose \( M \) has continuous sample paths and satisfies \( M_0 = 0 \) and \( [M,M]_t = t \) for all \( t\geq 0 \). Then \( M \) is a Brownian motion.
\end{theorem}

\subsection{Brownian Bridge}
\begin{definition}[Gaussian Process]
A Gaussian process \( X = (X_t)_{t\geq 0} \) is any process that, for arbitarty times \( t_1<t_2<\cdots<t_n \), the joint distribution of \( (X_{t_1},X_{t_2},\ldots, X_{t_n}) \) is jointly normal.
\end{definition}

\begin{definition}[Brownian Bridge (Version 1)]
Let \( W = (W_t)_{t\geq 0} \) be a Brownian motion and fix \( T>0 \). We define a Brownian bridge from \( a \) to \( b \) on \( [0,T] \), \( X^{a\to b} = (X_t^{a\to b})_{t\in [0,T]} \), by,
\begin{align*}
    X_t^{a\to b} = a+\frac{t}{T}(b-a)+W_t-\frac{t}{T}W_T, && t\in[0,T]
\end{align*}
\end{definition}

\begin{definition}[Brownian Bridge (Version 2)]
Let \( W = (W_t)_{t\geq0} \) be a Brownian motion and fix \( T>0 \). We defined a Brownian bridge from \( a \) to \( b \) on \( [0,T] \), \( Y^{a\to b} = (Y_t^{a\to b})_{t\in[0,T]} \), by,
\begin{align*}
    Y_t^{a\to b} = a + \dfrac{t}{T}(b-a)+(T-t) \int_0^t \dfrac{1}{T-s}\d W_s, && t\in[0,T]
\end{align*}
\end{definition}

\note{OTHER DEFINITIONS AND THEOREMS}

\subsection{Girsanov's Theorem}

\begin{definition}[Radon--Nikodym Derivative Process]
Let \( (\Omega,\cF,\PP) \) be a probability space, and let \( \bF = (cF_t)_{t\in[0,T]} \) be a filtration on this space. A Radon--Nikodym derivative process\( (Z_t)_{t\in[0,T]} \) is any process of the form,
\begin{align*}
    Z_t := \EE[Z | \cF_t]
\end{align*}
where \( Z \) is a random variable satisfying \( Z>0 \) and \( \EE[Z] = 1 \).
\end{definition}

\begin{lemma}
A Radon--Nikodym derivative process is a martingale.
\end{lemma}

\begin{proof}
For \( 0<s<t<T \) we have,
\begin{align*}
    \EE[Z_t | \cF_s] = \EE[\EE[Z |\cF_t]|\cF_s] = \EE[Z|\cF_s]  = Z_s \tag*{\qed}
\end{align*}
\end{proof}

\begin{lemma}
Let \( (Z_t)_{t\in[0,T]} \) be a Radon-Nikodym derivative process and define \( \d \tilde{\PP}/\d \PP = Z \). Suppose \( Y\in\cF_s \) where \( s\in[0,T] \). Then,
\begin{align*}
    \tilde{\EE}[Y] = \EE[Z_sY]
\end{align*}
\end{lemma}

\begin{proof}
We have,
\begin{align*}
    \tilde{\EE}[Y] = \EE[ZY] = \EE[Y\EE[Z|\cF_s]] = \EE[Z_sY] \tag*{\qed}
\end{align*}
\end{proof}

\note{I guess this makes sense that it shouldn't matter what \( s \) is since \( Z_t \) is a margingale}

\begin{lemma}
Let \( (Z_t)_{t\in[0,T]} \) be a Radon--Nikodym derivative process and define \( \d \tilde{\PP}/\d \PP = Z \). Suppose \( Y\in\cF_t \) where \( t\in[0,T] \) and let \( s\in [0,t] \). Then,
\begin{align*}
    \tilde{\EE}[Y|\cF_s] = \frac{1}{Z_s} \EE[Z_tY|\cF_s]
\end{align*}
\end{lemma}

\begin{theorem}[Girsanov]
Let \( W = (W_t)_{t\in[0,T]} \) be a Brownian motion on a probability space \( (\Omega,\cF,\PP) \) and let \( \bF = (\cF_t)_{t\in[0,T]} \) be a filtration for \( W \). Suppose \( \Theta = (\Theta_t)_{t\in[0,T]} \) is adapted to the filtration \( \bF \). Define \( (Z_t)_{t\in[0,T]} \) and \( \tilde{W} = (\tilde{W}_t)_{t\in[0,T]} \) by,
\begin{align*}
    Z_t = \exp \left( -\int_0^t \frac{1}{2} \Theta_s^2\d s - \int_0^t\Theta_s\d W_s \right)
    ,&& \d \tilde{W}_t = \Theta_t\d t + \d W_t
    ,&& \tilde{W}_0 = 0
\end{align*}
Assume that,
\begin{align*}
    \EE \left[ \int_0^T \Theta_t^2Z_t^2\d t \right] < \infty
\end{align*}
Define a Radon-Nikodym derivative \( Z = \d \tilde{\PP}/\d \PP = Z_T \). Then the process \( \tilde{W} \) is a Brownian motion under \( \tilde{P} \).
\end{theorem}


\begin{theorem}[Martingale Representation]
Let \( W = (W_t)_{t\in[0,T]} \) be a Brownian motion on a probability space \( (\Omega,\cF,\PP) \) and let \( \bF = (\cF_t)_{t\in[0,T]} \) be a filtration generated by \( W \). Let \( M \) be a martingale with respect to \( \bF \). Then there exists an \( \bF \)-adapted process \( \Gamma = (\Gamma_t)_{t\in[0,T]} \) such that,
\begin{align*}
    M_t = M_0 + \int_0^t \Gamma_s\d W_s, && t\in[0,T]
\end{align*}
\end{theorem}



\section{SDEs and PDEs}

\subsection{Stochastic Differential Equations}
\begin{definition}[Stochastic Differential Equation]
A stochastic differential equation (SDE) is an equation of the form,
\begin{align*}
    \d X_s = \mu(s,X_s)\d s + \sigma(s,X_s) \d W_s, && X_t = x
\end{align*}
where we refer to the functions \( \mu \) and \( \sigma \) as the drift and diffusion respectively, and call \( X_t = x \) the initial condition.
\end{definition}

\begin{definition}[Strong Solution]
A strong solution of an SDE is a stochastic process \( X = (X_s)_{s\geq t} \) such that,
\begin{align*}
    X_T = x + \int_t^T \mu(s,X_s)\d s + \int_t^T \sigma(s,X_s) \d W_s, && \forall T\geq t
\end{align*}
\end{definition}

A strong solution means that given a sample path of the Brownian motion, we can construct  a unique sample path of the solution.

\begin{theorem}
Consider the SDE,
\begin{align*}
    \d X_t = \mu(t,X_t)\d t + \sigma(t,X_t)\d W_t
\end{align*}
Suppose \( \mu \) and \( \sigma \) satisfy,
\begin{align*}
    |\mu(t,x)| + |\sigma(t,x)| &< C_1 (1+|x|)
\end{align*}\vspace{-1em}
\begin{align*}
    |\mu(t,x)-\mu(t,y)| + |\sigma(t,x)+\sigma(t,y)| &< C_2|x-y|
\end{align*}
for all \( t,x,y \) and some constant \( C_1 \) and \( C_2 \). Then the SDE has a unique solution which is adapted to the filtration generated by \( W \) and satisfies \( \EE \int_0^T X_t^2\d t <\infty \) for all \( T<\infty \).
\end{theorem}

\begin{theorem}[Markov Property of Solutions of SDEs]
Let \( X = (X_t)_{t\geq 0} \) be the solution of an SDE of the form,
\begin{align*}
    \d X_t = \mu(t,X_t)\d t + \sigma(t,X_t)\d W_t
\end{align*}
Then \( X \) is a Markov process. That is, for \( t\leq T \) and for some suitable function \( \varphi \), there exists a function \( g \) (dependent on \( t,T \), and \( \varphi \)) for which,
\begin{align*}
    \EE[\varphi(X_T) | \cF_t] = g(X_t)
\end{align*}
where \( \bF = (\cF_t)_{t\geq 0} \) is any filtration to which \( X \) is adapted.
\end{theorem}

\subsection{Connection to Partial Differential Equations}

\begin{theorem}
Let \( X = (X_t)_{t\geq 0} \) be the solution of an SDE of the form,
\begin{align*}
    \d X_t = \mu(t,X_t)\d t + \sigma(t,X_t)\d W_t
\end{align*}
For some suitible function \( \varphi \), define,
\begin{align*}
    u(t,X_t):= \EE[\varphi(X_T)|\cF_t]
\end{align*}
If \( u\in C^{1,2} \) then it satisfies the Kolmogorov Backward Equation,
\begin{align*}
    (\partial_t \cA(t)) u(t,\cdot) = 0, && u(T,\cdot) = \varphi, && \cA(t) = \mu(t,x) \partial_x + \frac{1}{2}\sigma^2(t,x)\partial^2_x
\end{align*}

\end{theorem}

\subsubsection{Killing a Diffusion}
\note{IDK why this example was relevant}

\subsection{Kolmogorov Forward and Backward Equations}
\begin{definition}[Transition Density]
The transition density of a process \( X \) is,
\begin{align*}
    \Gamma(t,x;T,y) \d y = \PP(X_T\in\d y | X_t = x)
\end{align*}
\end{definition}

\begin{definition}[Two Parameter Semigroup]
The two parameter semigroup \( (\cP_t)_{t\in[0,T]} \) of a Markov diffusion \( X \) is defined as,
\begin{align*}
    \cP(t,T)\varphi(x) = \EE[\varphi(X_T) | X_t = x] = \int\Gamma(t,x;T,y)\varphi(y) \d y
\end{align*}
where \( \varphi \) is integrable with respect to the transition density \( \Gamma \).
\end{definition}


\begin{theorem}
The semigroup satisifes,
\begin{align*}
    \cP(t,t) = I, && \cP(t,s)\cP(s,T) = \cP(t,T), && t\in[0,T], s\in[t,T]
\end{align*}
\end{theorem}

\begin{definition}[Infinitesimal Generator]
The infinitesimal generator of a semigroup of operators \( (\cP(t,s))_{t\in[0,T]} \) is defined as,
\begin{align*}
    \cA(t) \varphi(x) &= \lim_{s\to t^+} \frac{1}{s-t} \left( \cP(t,s) \varphi(x) - \varphi(x) \right)
    \\&=  \lim_{s\to t^+} \frac{1}{s-t} \left( \EE[\varphi(X_s)|X_t = x] - \varphi(x) \right)
\end{align*}
where \( \varphi \) is any function for which this limit exists.
\end{definition}

\begin{theorem}
If \( \varphi\in C^2_0 \) (bounded and twice differentiable) and \( X \) is the solution of,
\begin{align*}
    \d X_t = \mu(t,X_t) \d t + \sigma(t,X_t)\d W_t
\end{align*}
then the generator \( \cA(t) \) of the semigroup \( (P(s,t))_{t\in[0,T]} \) of \( X \) is given by,
\begin{align*}
    \cA(t) = \mu(t,X_t)\partial_x + \frac{1}{2}\sigma^2(t,X_t)\partial_x^2
\end{align*}
\end{theorem}

\begin{theorem}[KBE and KFE]
Let \( X = (X_t)_{t\geq 0} \) be the unique solution (assumed to exist) of,
\begin{align*}
    \d X_t = \mu(t,X_t) \d t + \sigma(t,X_t) \d W_t
\end{align*}
Let \( \Gamma \) denote the transition density of \( X \),
\begin{align*}
    \Gamma(t,x;T,y) \d y = \PP(X_T\in \d y | X_t = x)
\end{align*}

Then, seen as a function of the backwards variables \( (t,x) \), the transition density \( \Gamma(\cdot, \cdot,;T,y) \) satisfies the KBE,
\begin{align*}
    (\partial_t+\cA(t))\Gamma(\cdot, t; T,y) = 0, && \Gamma(T,\cdots; T,y) = \delta_y
\end{align*}
where \( \cA(t) \) is the infinitesimal generator of \( X \),
\begin{align*}
    \cA(t) = \mu(t,x)\partial_x + \frac{1}{2}\sigma^2(t,x)\partial_x^2
\end{align*}

Seen as a function of the forward variables \( (T,y) \), the transition density \( \Gamma(t,x;\cdot,\cdot) \) satisfies the KFE,
\begin{align*}
    (-\partial_T+\cA^*(T))\Gamma(t,x;T,\cdot) = 0, && \Gamma(t,x;t,\cdot) = \delta_x
\end{align*}
where \( \cA^*(T) \) is the \( L^2(\d x) \) adjoint of \( \cA(T) \),
\begin{align*}
    \cA^*(T) = -\partial_y \mu(T,y) + \frac{1}{2}\partial_y^2 \sigma(T,y)
\end{align*}
\end{theorem}

\begin{theorem}
    Let \( X = (X_t)_{t\geq 0} \) be defined on some interval \( I = (l,r) \) and \( \tau = \operatorname{inf}\{t \geq 0 : X_t \notin I\} \). Suppose \( X \) satisfies,
\begin{align*}
    \d X_t = \mu(X_t) \d t + \sigma(X_t)\d W_t
\end{align*}
Define,
\begin{align*}
    u(x) := \EE[\exp(-\lambda(\tau-t))\varphi(X_\tau) + \int_t^\tau \exp(-\lambda(s-t))g(X_s)\d s | X_T = x], && t\leq \tau
\end{align*}
Then the function \( u \) satisfies,
\begin{align*}
    (\cA - \lambda)u + g = 0 && \text{in } I \\
    u = \varphi && \text{on } \partial I
\end{align*}
    where \( \cA = \mu(X)\partial_x + \frac{1}{2} \sigma^2(x)\partial_x^2 \) is the infinitesimal generator of \( X \).
\end{theorem}

\begin{lemma}
The Laplace transform of \( \tau \), given by,
\begin{align*}
    u(x) := \EE[\exp(-\lambda t) | X_0 = x]
\end{align*}
satisfies,
\begin{align*}
    (\cA - \lambda)u = 0 && \text{in }I\\
    u=1 && \text{on }\partial I
\end{align*}

\end{lemma}

\subsection{Scalar Time-Homogeneous Diffusions}

Consider a diffusion \( X = (X_t)_{t\geq 0} \) that lives on some interval \( I \) with endpoints \( l \) and \( j  \), where \( -\infty \leq l < r\leq \infty \). Suppose \( X \) satisfies,
\begin{align*}
    \d X_t = \mu(X_t)\d t + \sigma(X_t) \d W_t
\end{align*}

The generator \( \cA \) of \( X \) is given by,
\begin{align*}
    \cA = \mu(x)\partial_x + \frac{1}{2}\sigma^2(x)\partial_x^2
\end{align*}

We can write this in the form,
\begin{align*}
    \cA = \frac{1}{m(x)} \partial_x \left( \frac{1}{s(x)} \partial_x \right)
\end{align*}
where \( s \) and \( m \) are the speed and scale densities, and are given by,
\begin{align*}
    s(x) = \exp \left( -\inf \frac{2\mu(x)}{\sigma^2(x)} \d x \right)
    ,&&
    m(x) = \frac{2}{\sigma^2(x)} \exp \left( \int \frac{2\mu(x)}{\sigma^2(x)} \d x \right)
\end{align*}

If \( m(x) \) is normalizable then \( m \) is a time-homogeneous solution of the KBE. Thus, \( m   \) is a stationary density for \( X \).

If we define a scale function \( S(x) = \int s(x) \d x \), then the process \( S(X) \) is a martingale.

\subsubsection{Boundary Classification and Boundary Conditions}

\begin{definition}[Scale Measure]
    Let \( X \) be a time-homogeneous scalar diffusion on the interval \( I = (l,r) \). Let \( s(x) \) be the scale density of \( X \).
For \( x,y\in (l,r) \) define,
\begin{align*}
    S[x,y] = \int_x^y s(u)\d u
    ,&&
    S(l,y] = \lim_{x\to l^+} S[x,y]
    ,&&
    S[x,r) = \lim_{y\to r^-} S[x,y]
\end{align*}
Define further,
\begin{align*}
    I_l = \int_l^r S(l,u] \d u, && I_r = \int_x^r S[u,r)\d u \\
    J_l = \int_l^x S[u,x] \d u, && J_r = \int_x^r S[x,u]\d u
\end{align*}
Where \( x \) is any point in \( (l,r) \). Note that for a given endpoint, whether \( I \) and \( J \) are finite does not depend on \( x \).
\end{definition}


\begin{definition}[Classification of boundaries]
Let \( X \) be a time-homogeneous scalar diffusion on the interval \( (l,r) \). An endpoint \( l \) or \( r \) is said to be,

\begin{table}[H]\centering
\begin{tabular}{r|cc}
    & \( I < \infty \) (attainable) & \( I = \infty \) (unattainable) \\ \hline
    \( J < \infty \) & regular & entrance \\
    \( J=\infty \) & exit & natural
\end{tabular}
\end{table}
\end{definition}

The process \( X \) can be started from a regular boundary and can reach a regular boundary in a finite time

The process \( X \) cannot be started from an exit boundary but can reach an exit boundary in finite time. If \( X \) reaches an exit boundary, it does not return.

The process \( X \) can be started from an entrance boundary but cannot reach an entrance boundary in finite time.

The process \( X \) cannot be started from a natural boundary nor can it reach a natural boundary in finite time.

\section{Jump Diffusions}

\subsection{Basic Definitions and Results on L\'evy Processes}

\begin{definition}[L\'evy Process]
    A \( d \)-dimensional stochastic process \( \eta = (\eta_t)_{t\geq 0} \), defined on a probability space \( (\Omega,\cF,\PP) \), is called a L\'evy process if it satisfies,
\begin{enumerate}[nolistsep]
    \item \( \eta_0 = 0 \).
    \item if \( 0\leq r < s<t<u<\infty \) then \( (\eta_u-\eta_t) \indep (\eta_s-\eta_r) \). 
    \item if \( 0\leq r < s< \infty \) then \( \eta_s-\eta_r \sim \eta_{s-r} \).
    \item for any \( \epsilon > 0 \) and \( t \geq 0 \), \( \lim_{s\to 0^+} \PP(|\eta_{t+s} -\eta_t| > \epsilon) = 0 \).
\end{enumerate}
\end{definition}

Note that the fourth condition only means that the probability of a jump occurring at a fixed time is zero.

We assume L\'evy processes are right continuous with left limits.

\begin{definition}[Filtration for L\'evy Process]
A filtration for the L\'evy process \( \eta \) is a collection of \( \sigma \)-algebras \( \bF = (\cF_t)_{t\geq 0} \) satisfying,
\begin{enumerate}
    \item information accumulates: if \( 0\leq s < t \) then \( \cF_s \subseteq \cF_t \).
    \item adaptivity: for all \( t\geq 0 \), \( \eta_t \in \cF_t \).
    \item independence of future increments: if \( u > t \geq 0 \) then \( (\eta_u-\eta_t) \indep \cF_t \).
\end{enumerate}    
\end{definition}

\begin{definition}[Jump of L\'evy Process]
The jump of a L\'evy process \( \eta \) at time \( t \) is defined as,
\begin{align*}
    \Delta \eta_t = \eta_t - \eta_{t^-}, && \eta_{t^-} = \lim_{s\to t^{-}} \eta_s
\end{align*}
\end{definition}

\begin{definition}[Poisson Random Measure]
    We define the Poisson random measure of \( \eta \), \( N: \RR_+ \times \cB_0^d \times \Omega \to \ZZ_{\geq 0} \) by,
    \begin{align*}
        N(t,U,\omega) = \sum_{s\in[0,t]} \bOne_{\Delta\eta_s(\omega)\in U}, && U \in \cB_0^d = \{ B\in \cB(\RR^d) : \{0\} \notin B\}
    \end{align*}
\end{definition}

The Poisson random measure counts the number of jumps of sizes in \( U \) which occur prior to time \( t \).

The differential form, \( N(\d t, \d z) \) counts the number of jumps of size in \( [z,z+ \d z] \) over the time interval \( \d t \).

\note{It is z+dz right?}

\begin{definition}[L\'evy Measure]
Let \( N \) be the Poisson random measure of a L\'evy Process \( \eta \). Define the L\'evy measure of \( \eta \), \( \nu: \cB_0^d \to \RR_+ \) by,
\begin{align*}
    \nu(U) = \EE[N(1,U)], && U\in \cB_0^d
\end{align*}
\end{definition}

\begin{theorem}
    Let \( U\in\cB_0^d \). Then the process \( (N(t,U))_{t\geq 0} \) is a Poisson process with intensity \( \nu(U) \).
\end{theorem}

We see that \( \nu(U) \) is the expected rate at which \( \eta \) has a jump of size \( z \in U \).

A pure jump L\;evy process has a finite Levy measure \( \nu(\RR\setminus \{0\} ) < \infty \) if and only if it can be represented by a compound Poisson process. In this case we can express \( \eta \) in as either,
\begin{align*}
    \eta_t  = \int_{\RR^d\setminus\{0\}} z N(t,\d z) && \text{or} && \eta_t = \sum_{t=1}^{P_t} X_n
\end{align*}

However, there are some L\'evy processes for which \( \nu(\RR\setminus \{0\} ) = \infty \). In this case neither of the prior representations makes sense.

\note{WHY THO??}

\begin{definition}[Compensated Poisson Random Measure]
    Let \( N \) be a Poisson random measure with associated L\'evy measure \( \nu \). The compensated Poisson random measure, denote \( \tilde{N} \) is defined as,
    \begin{align*}
        \tilde{N}(t,U) = N(t,U) - \nu(U) t
    \end{align*}
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                563 Stuff                %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagebreak
\section{Dynamic Mode Decomposition}
We would like to study some dynamical system,
\begin{align*}
    \dd{x}{t} = f(x,t), && x(t)\in\RR^n
\end{align*}

In discrete time we have a flow map,
\begin{align*}
    x_{k+1} = F(x_k)
\end{align*}

We would like to approximate \( x_t = f(x,t) \) by \( u_t = \cA x \). Given such an approximation we have,
\begin{align*}
    x_{k+1} = Ax_k, && A = \exp(\cA \Delta t)
\end{align*}


DMD finds \( A \) such that,
\begin{align*}
    \norm{x_{k+1} - Ax_k}
\end{align*}
is minimized over \( k=1,2,\ldots, m-1 \).

We can write this as,
\begin{align*}
    \min_A \norm{X' - AX}_F
\end{align*}
where,
\begin{align*}
    X = \left[\begin{array}{cccc}| & | & & |\\x_1 & x_2 & \cdots & x_{m-1} \\ |& | & & |\end{array}\right]
    ,&&
    X' = \left[\begin{array}{cccc}| & | & & |\\x_2 & x_3 & \cdots & x_{m} \\ |& | & & |\end{array}\right] 
\end{align*}

The solution to this minimization problem is,
\begin{align*}
    A = X' X^\dagger
\end{align*}

We are interested in the eigenvalues and eigenvectors of \( A \). Suppose \( A\Phi = \Phi \Lambda \).

Then,
\begin{align*}
    x(t) \approx \Phi\exp(\Omega t)\Phi^\dagger x(0), && \omega_j = \ln(\lambda_j)/\Delta t
\end{align*}

More usefully,
\begin{align*}
    x_{k+1} = A x_k
\end{align*}


Since \( A \) may be very large, we would like to take advantage of low rank structure of \( X \) in order to find eigenmodes for \( A \).

\iffalse
\subsubsection{Finding Eigenvalues of a Large Matrix using a Smaller One}
Consider the unitariy similarity transform,
\begin{align*}
    U^*AU, && U = \left[\begin{array}{c|c} \\ \hat{U} & \tilde{U} \\ &  \end{array}\right]
\end{align*}

Expanding we find,
\begin{align*}
    U^*AU = \left[\begin{array}{cc} \hat{U}^*A\hat{U} & \hat{U}^*A \tilde{U} \\ \tilde{U}^*A\hat{U} & \tilde{U}^*A\tilde{U} \end{array}\right]
\end{align*}

Suppose \( w \) is an eigenvalue of \( \hat{U}^*A\hat{U} \) with eigenvalue \( \lambda \). Then,
\begin{align*}
    U^*AU \left[\begin{array}{c} w \\ 0\end{array}\right] = \left[\begin{array}{c} \hat{U}^*A\hat{U} w \\ \tilde{U}^*A\hat{U} w\end{array}\right]
\end{align*}

If \( A = \hat{U} C \) or \( C \tilde{U}^* \) for any \( C \), then \( [w ; 0] \) is an eigenvector of \( U^*AU \) with eigenvalue \( \lambda \).

Therefore \( U[w;0] = \hat{U}w \) is an eigenvalue of \( A \) with eigenvalue \( \lambda \).
\fi

\subsection{DMD via SVD}

Let the (reduced) SVD of \( X \) be given by,
\begin{align*}
    X = \hat{U} \hat{\Sigma}\hat{V}^*, && \hat{U},\hat{V} \in \RR^{n\times r}, \hat{\Sigma}\in\RR^{r\times r}
\end{align*}


Then,
\begin{align*}
    A = X'X^\dagger = X'\hat{V}\hat{\Sigma}^{-1} \hat{U}^* 
\end{align*}

Define,
\begin{align*}
    \hat{S} := \hat{U}^*A\hat{U} = \hat{U}^*X' \hat{V}\hat{\Sigma}^{-1}
\end{align*}

Now note that \( \hat{S} \in \RR^{r\times r} \) so that it may be much easier to compute the eigenvalues of \( \hat{S} \) than of \( A \).

Moreover, \( A \) has rank \( r \) so all eigenvalues of \( A \) which are not eigenvalues of \( \hat{S} \) are zero.

Suppose \( w \) is an eigenvalues of \( \hat{S} \). Then,
\begin{align*}
    \lambda w = \hat{S}w = \hat{U}^*A\hat{U}w 
    &&\Longrightarrow&&
    \hat{U}\hat{U}^* A\hat{U}w = \lambda \hat{U} w
\end{align*}

The \( \hat{U}w \) is an eigenvector of \( A \) if \( A \) lies in the range of \( X \), since \( \hat{U}\hat{U}^* \) is an orthogonal projector to the range of \( X \).

In this case,
\begin{align*}
    \hat{\varphi} = \hat{U} w 
    = \frac{1}{\lambda} A \hat{U} w 
    = \frac{1}{\lambda} X'\hat{V}\hat{\Sigma}^{-1} \hat{U}^* \hat{U} w
    = \frac{1}{\lambda} X'\hat{V}\hat{\Sigma}^{-1} w
\end{align*}

Let \( \varphi = \frac{1}{\lambda} X'\hat{V}\hat{\Sigma}^{-1} w \). Then,
\begin{align*}
    A\varphi
    = A (X'\hat{V}\hat{\Sigma}^{-1})w 
    = (X'\hat{V}\hat{\Sigma}^{-1} \hat{U}^*)X'\hat{V}\hat{\Sigma}^{-1} w
    = \varphi\hat{S} w
    = \lambda \varphi w
\end{align*}

Note that this is true regardless of whether \( A \) lies in the range of \( X \).


\subsection{Projective vs. Exact DMD}
We now have two different ways of defining DMD modes.
We call \( \hat{\varphi} = \hat{U} w \) a projective DMD modes, and \( \varphi = \frac{1}{\lambda} X'\hat{V}\hat{\Sigma}^{-1} \) an exact DMD mode.

Projective DMD produces modes contained in the range of \( X \), while exact DMD modes lie in the range of \( Y \).

As noted above, the exact DMD modes are always eigenvectors of \( A \). On the other hand, the projective DMD modes are eigenvectors of \( A \) when \( \hat{U}\hat{U}^*A = A \).


\subsection{DMD with Rank Truncation}

We have seen that we can use the reduced SVD to compute the DMD modes. We can instead use a truncated SVD to approximate the DMD modes. This will produce low rank approximations to the DMD modes. How good of approximations these are depends on the singular values of \( X \).

\note{DO we have any way of knowing how good though?}

\note{Is projective ever better in the approximate case?}


\subsection{Implementations}

\begin{method}[Dynamic Mode Decomposition]
\begin{enumerate}
    \item Arrange data \( (x_1,x_1'), (x_2, x_2'), \ldots, (x_m,x_m')\) into \( X \) and \( X' \)
    \item Compute (reduced/rank-truncated) SVD \( X = U\Sigma V^* \)
    \item Define, \( \tilde{S} = U^*X'V\Sigma^{\dagger} \)
    \item Compute eigen-decompoition, \( \tilde{S} W = W\Lambda \)
    \item Define DMD modes \( \Phi = X'V\Sigma^\dagger W \)
        \note{scale by \( \Lambda \)?}
    \item Approximate solution is \( x(t) = \Phi e^{\Omega t} \Phi^\dagger x(0) \), where \( \omega_i = \ln(\lambda_i)/\Delta t \).
\end{enumerate}
\end{method}



\section{Koopman Embedding}


\subsection{Example}

Start with nonlinear system \( \dot{x} = f(x) \).
\begin{align*}
    \dd{}{t} \left[\begin{array}{c}x_1 \\ x_2\end{array}\right]
        = 
    \left[\begin{array}{c}\mu x_1 \\ \lambda(x_2-x_1^2) \end{array}\right]
\end{align*}

Define \( y_1 = x_1 \), \( y_2 = x_2 \), \( y_3 = x_1^2 \). Then,
\begin{align*}
    \dd{}{t} \left[\begin{array}{c}y_1 \\ y_2 \\ y_3 \end{array}\right]
        =
    \left[\begin{array}{c} \mu x_1 \\ \lambda (x_2-x_2^2) \\ 2 x_1 \dot{x_1} \end{array}\right]
        =
    \left[\begin{array}{c} \mu y_1 \\ \lambda (y_2 - y_3) \\ 2\mu y_3 \end{array}\right]
        =
    \left[\begin{array}{ccc}\mu & 0 & 0 \\ 0 & \lambda & -\lambda \\ 0 & 0 & 2\mu \end{array}\right]
    \left[\begin{array}{c} y_1 \\ y_2 \\ y_3\end{array}\right]
\end{align*}

In our new coordinates we now have a linear system \( \dot{y} = K y  \). 


\note{something about fixed points}

    

\section{Time Delay Embedding}


\section{SINDy}



\bibliography{stochastics}{}
\bibliographystyle{ieeetr}


\end{document}
