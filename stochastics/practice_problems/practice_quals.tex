\begin{problem}[Practice Exam 1, Problem 1]
    Let \( X = (X_n)_{n\in\NN_0} \) be a discrete time Markov chain with \( X_n \) representig the amount of water in a reservoir at noon on day \( n \). Assume \( X_0 \in \NN_0 \). Let \( Y = (Y_n)_{n\in\NN_0} \) be a sequence of iid random variables with \( Y_n \) representing the aount of water that flows into the reservoir during the \( n \)-th day. The state space of \( Y \) is \( \{0,1,2,\ldots \} \). The resevoir has a maximum capacity of \( K\in\NN \). When the resevoir is filled to level \( K \), all excssive inflows are lost.
    \begin{enumerate}[nolistsep,label=(\alph*)]
        \item Write the one-step transition matrix \( P \) of \( X \) in terms of the probability generating function \( G_Y \) of \( Y \).
        \item Find an expression for the stationary distribution \( \pi \) of \( X \) in terms of the probability generating function \( G_Y \) of \( Y \).
    \end{enumerate}
\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item 
        We assume all the water comes in the afternoon. That is, \( X_{n+1} = X_n + Y_n \).

        Suppose on day \( n \) the resevoir is not full. That is, \( X_n = k < K \). If it is not filled completely by the incoming water, then some amount of water \( j < K-k \) was added. In this case \( X_{n+1} = k+j \) with probability,
        \begin{align*}
            \PP(Y_n = j) = f_Y(j) = 
            \left[\frac{1}{j!}\dd[j]{G_Y(s)}{s} \right]_{s=0}
        \end{align*}
        
        Otherwise, \( X_{n+1} = K \) with probability,
        \begin{align*}
            1-\sum_{j < K-k} f_Y(j) = 1 - \sum_{j<K-k} \left[\frac{1}{j!} \dd[j]{G_Y(s)}{s} \right]_{s=0}
        \end{align*}
        
        Suppose \( X_n = K \). Then since no water leaves the resevoir, \( X_{n+1} = K \) with probability one.

        We can write this as,
        \begin{align*}
            X_{n+1} = \begin{cases}
                \left[\frac{1}{j!}\dd[j]{G_Y(s)}{s} \right]_{s=0} & j < K - X_n \\ \\
                1 - \sum_{j<K-X_n} \left[\frac{1}{j!}\dd[j]{G_Y(s)}{s} \right]_{s=0} & \text{otherwise}
            \end{cases}
        \end{align*}
        
    \item
        Note that \( \pi = [0,0,\ldots,0,1] \) is a stationary distribution.

        \note{argue the distributoin is unique?}


        \note{alternative approach??}
        Clearly \( X_n \to K \) as \( n\to\infty \).

        \note{in what sense?}
        
\end{enumerate}
\end{solution}

\begin{problem}[Practice Exam 1, Problem 2]
    Let \( (X,Y) = (X_t,Y_t)_{t\geq 0} \) satisfy the following SDE,
    \begin{align*}
        \d X_t = \d W_t^1, && \d Y_t = \d W_t^2, && (X_0,Y_0) = (x,y)
    \end{align*}
    where \( W = (W_t^1,W_t^2)_{t\geq 0} \) is a two-dimensinoal Brownian motion with independent components. Define a process \( (R,\Phi) = (R_t,\Phi_t)_{t\geq 0} \) as follows,
    \begin{align*}
        \Phi_t = \arctan(Y_t/X_t), && R_t^2 = X_t^2 + Y_t^2
    \end{align*}
    \begin{enumerate}[nolistsep,label=(\alph*)]
        \item Derive the SDEs satisfied by \( (R,\Phi) \).
        \item Define,
            {\small
            \begin{align*}
                u(r,\phi) = \EE \left[ e^{-\lambda \tau} f(R_\tau) | R_0 = r,\Phi_0 = \phi \right], &&
                \tau = \inf\{t\geq 0:\Phi_t\notin(0,\pi/2)\}, &&
                \phi \in (0,\pi/2)
            \end{align*}
            }
            Derive a PDE satisfied by \( u \).
        \item Desribe with pseudo-code how you would find \( u(r,\phi) \) using Monte Carlo simulation.
    \end{enumerate}    
\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item Define \( f(x,y) = \arctan(y/x) \) and \( g(x,y) = \sqrt{x^2+y^2} \). Now note that,
        \begin{align*}
            \Phi_t = f(X_t,Y_t), && R_t = g(X_t,Y_t)
        \end{align*}

        Appying It\^o's formula we find,
        \begin{align*}
            \d \Phi_t &=  f_x(X_t,Y_t)\d X_t + f_y(X_t,Y_t)\d Y_t 
            \\&\hspace{3em}+ \frac{1}{2}\big(f_{xx}(X_t,Y_t)\d[X,X]_t + f_{xy}(X_t,Y_t)\d [X,Y]_t 
            \\&\hspace{6em}+ f_{yx}(X_t,Y_t)\d[Y,X]_t + f_{yy}(X_t,Y_t)\d[Y,Y]_t\big) 
        \end{align*}
        
        Using our Heuristics we have,
        \begin{align*}
            \d[X,X]_t = \d[Y,Y]_t = \d t, && \d[X,Y]_T = \d[Y,X]_t = 0
        \end{align*}
        
        We compute,
        \begin{align*}
            f_x(x,y) &= -\frac{y}{x^2+y^2} = - \frac{\sin(\arctan(y/x)}{\sqrt{x^2+y^2}} \\
            f_y(x,y) &= \frac{x}{x^2+y^2} = \frac{\cos(\arctan(y/x))}{\sqrt{x^2+y^2}}\\
            f_{xx}(x,y) &= \frac{2xy}{(x^2+y^2)^2} \\ 
            f_{yy}(x,y) &= -\frac{2xy}{(x^2+y^2)^2}
        \end{align*}
        
        Therefore, maxing the substitutions, \( \Phi_t = \arctan(Y_t/X_t) \), and \( R_t = \sqrt{X_t^2+Y_t^2}  \),
        \begin{align*}
            \d \Phi_t &= - \frac{\sin(\Phi_t)}{R_t}\d W_t^1 + \frac{\cos(\Phi_t)}{R_t}\d W_t^2
        \end{align*}
        
        Similarly,
        \begin{align*}
            \d R_t &=  g_x(X_t,Y_t)\d X_t + g_y(X_t,Y_t)\d Y_t
            \\&\hspace{3em}+ \frac{1}{2}\big(g_{xx}(X_t,Y_t)\d[X,X]_t + g_{xy}(X_t,Y_t)\d [X,Y]_t 
            \\&\hspace{6em}+ g_{yx}(X_t,Y_t)\d[Y,X]_t + g_{yy}(X_t,Y_t)\d[Y,Y]_t\big) 
        \end{align*}
        
        We compute,
        \begin{align*}
            g_x(x,t) &= \frac{x}{\sqrt{x^2+y^2}} = \cos(\arctan(y/x)) \\
            g_y(x,t) &= \frac{y}{\sqrt{x^2+y^2}} = \sin(\arctan(y/x)) \\
            g_{xx}(x,t) &= \frac{y^2}{(x^2+y^2)^{3/2}} \\
            g_{yy}(x,t) &= \frac{x^2}{(x^2+y^2)^{3/2}}
        \end{align*}
        
        Therefore, maxing the substitutions, \( \Phi_t = \arctan(Y_t/X_t) \), and \( R_t = \sqrt{X_t^2+Y_t^2}  \),
        \begin{align*}
            \d R_t &= \cos(\Phi_t)\d W_t^1 + \sin(\Phi_t) \d W_t^2 + \frac{1}{2R_t} \d t
        \end{align*}


    \item
        \note{DO THIS SHIT}
    \item 

\end{enumerate}
\end{solution}


\begin{problem}[Practice Exam 2, Problem 1]
Let \( Y = (Y_n)_{n\in\NN_0} \) be a sequence of iid random variables with \( Y_n\sim \operatorname{Pois}(\lambda) \) representing the number of particles entering a chamer at time \( n \). The lifetimes of the particles are iid geometric random variables with parameter \( p \). Let \( X_n \) represent the number of particles in the chamber at time \( n \).
\begin{enumerate}[nolistsep,label=(\alph*)]
    \item Give an expression for \( p(i,j) = \PP(X_{n+1} = j | X_n = i) \).
    \item Find the stationary distribution \( \pi \) of \( X = (X_n)_{n\geq 0} \).
\end{enumerate}
\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item
        If the lifetime of a particle is a geometric random variable with parameter \( p \), then at each step there is a probability \( p \) that the particle will decay and a probability \( 1-p \) that the particle will not decay.

        Let \( Z_n \) represent the number of particles which no \textit{not} decay during the \( n \)-th step. That is,
        \begin{align*}
            X_{n+1} = Z_n + Y_n 
        \end{align*}
        
        Since each of the \( X_n \) particles no not decay with probability \( 1-p \) we have \( Z_n \sim \operatorname{Bin}(X_n,1-p) \) and \( Y_n \sim \operatorname{Pois}(\lambda) \).

        Denote the generating functions of \( Y_n \) and \( Z_n \) by, \( G_{Y_n}(s) \) and \( G_{Z_n}(s) \) respectively.
        Explicitly,
        \begin{align*}
            G_{Y_n}(s) = G_Y(s) = e^{\lambda(s-1)}, &&
            G_{Z_n}(s) = (p+(1-p)s)^{X_n} = G_{X_n}(p+(1-p)s)
        \end{align*}
        
        Assume \( Y_n \) is independent of \( X_n \) and therefore of \( Z_n \). 
        If \( X_n = i \) the generating function is \( G_{X_n} = s^i \). We can then write,
        \begin{align*}
            G_{X_{n+1}}(s) = G_{Y_n}(s)G_{Z_n}(s) = G_Y(s) (p+(1-p)s)^i = e^{\lambda(s-1)} (p+(1-p)s)^i
        \end{align*}

        Therefore,
        \begin{align*}
            p(i,j) &= \PP(X_{n+1} = j | X_n = i) 
            \\&= \left[\frac{1}{j!} \dd[j]{G_{X_{n+1}}(s)}{s} \right]_{s=0}
            \\&= \left[\frac{1}{j!} \dd[j]{}{s} \left[ e^{\lambda(s-1)}(p+(1-p)s)^i \right] \right]_{s=0}
        \end{align*}

       
    \item 
        More generally, the generating function \( G_{X_{n+1}}(s) \) of \( X_{n+1} \) is then,
        \begin{align*}
            G_{X_{n+1}}(s) = G_{Y_n}(s)G_{Z_n}(s) = G_{Y}(s)G_{X_{n}}(p+(1-p)s) 
        \end{align*}
        
        This gives a recurrence relation. We assume \( X_0 = 0 \) so that \( G_{X_0}(s) = 1 \).
        For convencience write \( q = 1-p \). Then,
        \begin{align*}
            1+q^k(s-1) |_{s=(1+q(s-1))} = 1+q^k((1+q(s-1))-1) = 1+q^{k+1}(s-1)
        \end{align*}
        
        Therefore,
        \begin{align*}
            G_{X_n}(s) &= G_Y(s) G_{X_{n-1}}(1+q(s-1)) \\
            &= G_Y(s) G_Y(1+q(s-1)) G_{x_{n-2}}(1+q^2(s-1)) \\
            &\hspace{.6em}\vdots \\
            &= \prod_{k=0}^n G_Y(1+q^k(s-1))
        \end{align*}
       
        We can rewrite this as,
        \begin{align*}
            G_{X_n}(s) = \exp \left( \sum_{k=0}^n \lambda((1+q^k(s-1))-1) \right)
            = \exp \left( \lambda(s-1) \sum_{k=0}^n q^k \right)
        \end{align*}
        
        Taking the limit as \( n\to\infty \) we find,
        \begin{align*}
            G_{X_{\infty}}(s) 
            &= \lim_{n\to\infty} G_{X_n}(s) 
            \\&= \exp \left( \lambda(s-1) \sum_{k=0}^{\infty} q^k \right) 
            \\&= \exp \left( \frac{\lambda(s-1)}{1-q}  \right) 
            \\&= \exp\left( \frac{\lambda}{p}(s-1)\right)        
        \end{align*}
        
        Therefore, by the continuity theorem, \( X_\infty \) is distributed like a Poisson random variable with parameter \( \lambda/p \). 

        This means the invariant distribution of \( X \) is the density function of a Poisson random variable with parameter \( \lambda/p \). That is,
        \begin{align*}
            \pi(k) = \left( \frac{\lambda}{p} \right)^k \frac{e^{-\lambda/p}}{k!}
        \end{align*}
        
\end{enumerate}
\end{solution}


\begin{problem}[Practice Exam 2, Problem 2]
    Fix a probability space \( (\Omega, \cF, \PP) \) and a filtration \( \bF = (\cF_t)_{0\leq t\leq T} \) where \( T< \infty \). Consider a process \( P = (P_t)_{0\leq t\leq T} \) defined as,
    \begin{align*}
        P_t = \EE[\bOne_{X_T\leq a} | \cF_t], && \d X_t = \d W_t
    \end{align*}
    where \( W \) is a \( (\PP,\bF) \)-Brownian motion. Derive an SDE for the process \( P \). Your answer should not involve \( X \). You may find it useful to use the CDF \( \Phi \) of a standard normal random variable,
    \begin{align*}
        \Phi(z) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{z} e^{-x^2/2}\d x
    \end{align*}
    its inverse \( \Phi^{-1} \) and its derivative \( \phi:= \Phi' \).
\end{problem}

\begin{solution}[Solution]
Note that condition on \( \cF_t \) is the same as conditioning on \( X_t \). For notational convenience let \( x=X_t \). Then,
\begin{align*}
    P_t = \EE[\bOne_{X_T\leq a} | \cF_t] = \PP(X_T\leq a | X_t = x)
    = \int_{-\infty}^a \Gamma(t,x;T;y)\d y
\end{align*}

Note further that,
\begin{align*}
    \Gamma(t,x;T,y) = \frac{1}{\sqrt{2\pi (T-t)}}\exp \left( -\frac{(y-x)^2}{2(T-t)} \right) 
    = \frac{1}{\sqrt{T-t}}\phi \left( \frac{y-x}{\sqrt{T-t}} \right)
\end{align*}

    Let \( u = (y-x)/\sqrt{T-t} \). Then \( \d y = \sqrt{T-t} \d u \) so,
    \begin{align*}
       P_t
        = \int_{-\infty}^{a}  \frac{1}{\sqrt{T-t}}\phi \left( \frac{y-x}{\sqrt{T-t}} \right)\d y
        = \int_{-\infty}^{\frac{a-x}{\sqrt{T-t}}} \phi(u)\d u
        = \Phi \left( \frac{a-x}{\sqrt{T-t}} \right)
    \end{align*}
    
Recall that \( X_t = x \) so,
\begin{align*}
    P_t = \Phi \left( \frac{a-X_t}{\sqrt{T-t}} \right)
   \end{align*}


    Let \( Y_t = (a-X_t)/\sqrt{T-t} = \Phi^{-1}(P_t) \). Then,
    \begin{align*}
        \d Y_t = \frac{1}{2(T-t)} \frac{a-X_t}{\sqrt{T-t}}\d t-\frac{1}{\sqrt{T-t}}\d X_t 
        = \frac{Y_t}{2(T-t)} \d t - \frac{1}{\sqrt{T-t}}\d W_t
    \end{align*}
    
    Therefore,
    \begin{align*}
        \d P_t &= \d \Phi(Y_t) = \phi(Y_t)\d Y_t + \frac{1}{2} \phi'(Y_t)\d[Y,Y]_t 
        \\&= \phi(Y_t) \frac{Y_t}{2(T-t)}\d t - \phi(Y_t) \frac{1}{\sqrt{T-t}}\d W_t + \frac{1}{2}\phi'(Y_t) \frac{1}{T-t}\d t
        \\&= \left(\phi(Y_t) \frac{Y_t}{2(T-t)} + \frac{1}{2}\phi'(Y_t) \frac{1}{T-t} \right)\d t  - \frac{\phi(Y_t)}{\sqrt{T-t}}\d W_t 
    \end{align*}
    We know that \( \phi'(z) = z\phi(z) \).
    (alternative argument: As \( P_t \) is a martingale we know the \( \d t \) term is zero.) 
    Therefore,
    \begin{align*}
        \d P_t = -\frac{\phi(\Phi^{-1}(P_t))}{\sqrt{T-t}}\d W_t
    \end{align*}
    


\end{solution}



\begin{problem}[Practice Exam 3, Problem 1]
Let \( X = (X_t)_{t\geq 0} \) be a continuous time Markov chain with \( X_t \) representing the number of individuals in a population at time \( t \). Individuals do not reproduce. However, immigrants join the population as a Poisson process with parameter \( \lambda \). The lifetimes of individuals are iid exponentially distributed random varaibles with parameter \( \mu \).
\begin{enumerate}[nolistsep,label=(\alph*)]
    \item Write the generator \( G \) of \( X \)
    \item Find the stationary distribution \( \pi \) of \( X \).
\end{enumerate}
\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item
        This is a M/M/\(\infty\) queue with arrival parameter \( \lambda \) and service parameter \( \mu \).
        
        In a short time \( s\) the probability of no immigrant joining the population is \( 1-\lambda s + \cO(s^2) \), the probability of one immigrant joining is \( \lambda s + \cO(s^2) \), and the probability more than one immigrant joining is \( \cO(s^2) \).

        Similarly, the probability of a given individual dying is \( \mu s + \cO(s^2) \) and the probability of an individual not dying is \( 1-\mu s + \cO(s^2) \). 

        The generator is then,
        \begin{align*}
            G = \left[\begin{array}{ccccccc}
            -\lambda & \lambda \\
            \mu & -(\mu+\lambda) & \lambda \\
            & 2\mu & - (2\mu+\lambda) & \lambda \\
            && 3\mu & -(3\mu+\lambda) & \lambda \\
                &&&\ddots&\ddots&\ddots 
            \end{array}\right]
        \end{align*}
       
       \item 
        An M/M/\(\infty\) queue with these parameters has invariant distribution,
        \begin{align*}
            \pi(k) = \frac{(\lambda/\mu)^k e^{-\lambda\mu}}{k!}
        \end{align*}
\end{enumerate}
\end{solution}


\begin{problem}[Practice Exam 3, Problem 2]
Fix a probability space \( (\Omega, \cF,\PP) \) and a filtratoin \( \bF = (\cF_t)_{t\geq 0} \). Consider a process \( X = (X_t)_{t\geq 0} \) that satisfies the following SDE,
\begin{align*}
    \d X_t = b\d t + a\d W_t
\end{align*}
where \( W \) is a \( (\PP,\bF) \)-Brownian motion. Suppose that \( X_0 \in (L,R) \) and that \( \{L\} \) and \( \{R\} \) are reflecting boundaries.
\begin{enumerate}[nolistsep,label=(\alph*)]
    \item Derive an expression for the invariant distribution of \( X \).
    \item Derive an expression for the transition density \( \Gamma(t,x;T,y)\d y:= \PP(X_t \in\d y | X_t = x) \).
    \item Show that \( \Gamma(t,x;T,y) \to \pi(y) \) as \( T\to\infty \).
\end{enumerate}

\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item 
    \item 
\end{enumerate}
\end{solution}

\begin{problem}[Practice Exam 4, Problem 1]
    A transition probablity matrix \( P \) for a Markov chain with \( N \) states is said be doubly stochastic if the entries in each of its columns add up to one.
\begin{enumerate}[nolistsep,label=(\alph*)]
\item Show that the uniform distribution given by \( q_i = 1/N \) for all \( j \) is a stationary distribution for such a Markov chain.
\item Consider the following random walk on the sets of integers \( \{0, 1, \ldots, L\} \). The walk jumps to the right or left at each step with probability 1/2 subject to the rule that if it tries to go to the left from 0 or to the right from \( L \) it stays put. Compute the stationary distribution of this random walk.
\item Consider the following random walk on state-space \( \{0, 1, 2, \ldots, L\} \) of numbers arranged on a ring. At each step, the walk goes to the right with probability \( a \) or to the left with probability \( 1 - a \) subject to the rules if it tries to go to the left from 0 it ends up at \( L \) or if it tries to go to the right from \( L \) it ends up at 0. Compute the stationary distribution of this chain.
\end{enumerate}
\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item By definition, since \( P \) is doubly stochastic, \( \sum_i P_{ij} = 1 \) for all \( j \). Trivially,
        \begin{align*}
            (qP)_j = \sum_{i=1}^N q_{i}P_{ij} = \sum_{i=1}^N \frac{1}{N} P_{ij} = \frac{1}{N}\sum_{i=1}^N P_{ij} = \frac{1}{N} = q_i
        \end{align*}

        This proves \( qP = q \). That is, \( q \) is a stationary distribution of \( P \).

    \item We have probability transition matrix,
        \begin{align*}
            P = \left[\begin{array}{cccccc}
                1/2 & 1/2 \\
                1/2 & &1/2 \\
                &1/2 & &1/2 \\
                &&1/2 &  &\ddots \\
                &&&\ddots &&1/2 \\ 
                &&&&1/2 & 1/2
            \end{array}\right]
        \end{align*}
       
        This is doubly stochastic so it has an invariant distribution,
        \begin{align*}
            \pi = [1/L,1/L,\ldots, 1/L]
        \end{align*}

        \note{Something about uniqueness, although I guess this isn't really needed based on problem wording}
        
    \item We have probability transition matrix,
        \begin{align*}
            P = \left[\begin{array}{cccccc}
                & 1/2 &&&& 1/2 \\
                1/2 & &1/2 \\
                &1/2 & &1/2 \\
                &&1/2 &  &\ddots \\
                &&&\ddots &&1/2 \\ 
                1/2 &&&&1/2 & 
            \end{array}\right]
        \end{align*}
       
        This is doubly stochastic so it has an invariant distribution,
        \begin{align*}
            \pi = [1/L,1/L,\ldots, 1/L]
        \end{align*}
        

\end{enumerate}
\end{solution}


\begin{problem}[Practice Exam 4, Problem 2]
    Fix a probability space \( (\Omega, \cF, \PP\) and a filtration \( \bF = (\cF_t )_{0\leq t \leq T} \). Suppose that \( X = (X_t)_{t\geq 0} \) satisfies the following SDE,
    \begin{align*}
        \d X_t = - \kappa X_t\d t + \sigma \d W_t
    \end{align*}
    where \( W \) is a \( (\PP,\bF) \)-Brownian motion. Now consider a chance of measure,
    \begin{align*}
        \dd{\tilde{\PP}}{\PP} = \exp \left( -\frac{1}{2}\gamma^2 T - \gamma W_T \right).
    \end{align*}
\begin{enumerate}[nolistsep,label=(\alph*)]
    \item Derive an SDE for theh process \( X \) under \( \tilde{\PP} \). Your answer should be given in terms of a process \( \tilde{W} \) which is a \( (\tilde{\PP},\bF) \)-Brownian motion.
    \item Define functions \( u:[0,T]\times \RR \to \RR \) and \( \tilde{u}:[0,T]\times \RR \to \RR \) as follows
        \begin{align*}
            u(t,x):= \EE[h(X_T) | X_t = x), && \tilde{u}(t,x):= \tilde{\EE}[h(X_T) | X_t = x]
        \end{align*}
        Provide the PDEs satisfied by \( u \) and \( \tilde{u} \), respectively.
\end{enumerate}


\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item 
    \item 
\end{enumerate}
\end{solution}

\begin{problem}[Practice Exam 5, Problem 1]
    Consider a Markov chain with state space \( \{0,1,2,\ldots\} \) and transition probabilities,
    \begin{align*}
        p(i,i+1) &= p_i, && i\geq 0 \\
        p(i,i-1) &= q_i, && i> 0 \\
        p(i,i) &= r_i, && i\geq 0
    \end{align*}
    For \( N > 0 \) and satet \( i \), let \( a_N(i) \) be the probability that the tiem of first visit to state \( N \) is strictly less than the time of first visit to state \( 0 \) if we start at state \( i \). Note that \( a_N(0) = 0 \) and \( a_N(N) = 1 \).
\begin{enumerate}[nolistsep,label=(\alph*)]
    \item Write a recursive relation for \( a_N(i) \) by consider what happens on the first transition out of state \( i \).
    \item Solve the above equation to compute \( a_N(i) \).
    \item Use (b) above to show that state 0 is recurrent if and only if,
        \begin{align*}
            \sum_{j=1}^{\infty} \prod_{i=1}^{j-1} \frac{q_i}{p_i} = \infty
        \end{align*}
    \item Analyze the situation where \( p_i = p \), \( q_i = 1-p \), \( r_i = 0 \) for \( x \geq 1 \), and \( r_0 = 1-p \).
\end{enumerate}

\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item 
    \item 
\end{enumerate}
\end{solution}


\begin{problem}[Practice Exam 5, Problem 2]
    Fix a probability space \( (\Omega,\cF,\PP) \) and a filtration \( \bF = (\cF_t)_{t\geq 0} \). Consider a mean-repelling OU process,
    \begin{align*}
        \d X_t = X_t\d t + \sqrt{2} \d W_t
    \end{align*}
    where \( W \) is a \( (\PP,\bF) \)-Brownian motion.
\begin{enumerate}[nolistsep,label=(\alph*)]
    \item Derive two representations of the transition density \( \Gamma(t,x,T,y)\d y := \PP(X_t \in\d y | X_t = x) \) of \( X \). One of the representations should involve Hermite polynomials.
    \item Does the process \( X \) have an invariant distribution? If so, provide it. If not, explain why not.
\end{enumerate}
\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item 
    \item 
\end{enumerate}
\end{solution}


\begin{problem}[Practice Exam 6, Problem 1]
Consider a game which is played on a network with nodes numbered \( 1, 2, 3, 4 \) and edges that connect these nodes as show below. Note that each node in this network has self-loop edges. Every edge in the network has an associated weight. For example, the weight associated with the edge connecting nodes 1 and 2 is 3. Similarly the weight associated with the self-loop edge on node 1 is 1. The game proceeds as follows. You have a token that you move randomly from one node to another with probabilities propotional to the corresponding edge weights. For example, the probability that your token moves from node 1 to node 2 is \( 3/(1 + 2 + 3) = 1/2 \). Let \( X_n \) be the position of your token after \( n \) moves, for \( n = 0, 1, 2, \ldots \).
\begin{center}
\begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,
  thick,main node/.style={circle,draw,minimum size=7mm}]

  \node[main node] (1) {1};
    \node[main node] (2) [right of=1] {2};
  \node[main node] (3) [below of=2] {3};
  \node[main node] (4) [below of=1] {4};

  \path[every node/.style={fill=white,inner sep=1pt}, every loop/.style={}]
    (1) edge[in=90,out=180, loop] node {1} (1)
        edge node {3} (2)
    (2) edge[in=0, out=90, loop] node {1} (2)
        edge node {4} (3)
    (3) edge[in=270, out=0, loop] node{1} (3)
        edge node {5} (4)
    (4) edge[in=270, out=180, loop] node{1} (4)
        edge node {2} (1)
        ;
\end{tikzpicture}
\end{center}
\begin{enumerate}[nolistsep,label=(\alph*)]
    \item Model the stochastic process \( X_n \) as a Markov chain by drawing its state transition diagram and write the corresponding one-step transition probability matrix.
    \item Calculate the stationary distribution that your token is at node \( j \), for \( j = 1, 2, 3, 4 \).
    \item Find the expected number of token moves between two consecutive visits to node 2.
\end{enumerate}
\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item We have transition diagram, 
    \begin{center}
    \begin{tikzpicture} 

    \draw node[circle, draw, minimum size=7mm] (1) at (0,3) {1};
    \draw node[circle, draw, minimum size=7mm] (2) at (3,3) {2};
    \draw node[circle, draw, minimum size=7mm] (3) at (3,0) {3};
    \draw node[circle, draw, minimum size=7mm] (4) at (0,0) {4};


        \path[->,every node/.style={fill=white,inner sep=1pt}, every loop/.style={->}]
    (1) edge[in=90,out=180, loop] node {1/6} (1)
        edge[bend left=30] node {1/2} (2)
        edge[bend left=30] node {1/3} (4)
    (2) edge[in=0, out=90, loop] node {1/8} (2)
        edge[bend left=30] node {1/2} (3)
        edge[bend left=30] node {3/8} (1)
    (3) edge[in=270, out=0, loop] node{1/10} (3)
        edge[bend left=30] node {1/2} (4)
        edge[bend left=30] node {2/5} (2)
    (4) edge[in=270, out=180, loop] node{1/8} (4)
        edge[bend left=30] node {1/4} (1)
        edge[bend left=30] node {5/8} (3)
        ;
    \end{tikzpicture}
    \end{center}

        The corresponding probability transition matrix \( P \) is,
        \begin{align*}
            P = \left[\begin{array}{cccc}
                1/6 & 1/2 & 0 & 1/3 \\
                3/8 & 1/8 & 1/2 & 0 \\
                0 & 2/5 & 1/10 & 1/2 \\
                1/4 & 0 & 5/8 & 1/8
            \end{array}\right]
        \end{align*} 
    \item 
        We easily compute the right eigenvector corresponding to eigenvalue \( 1 \) as,
        \begin{align*}
            \pi = \frac{1}{4}[3/4,1,5/4,1] = [3/16,1/4,5/16,1/4]
        \end{align*}
        
    \item The expected number of moves is the mean recurrence time. This chain is irreducible so by theorem we have,
        \begin{align*}
            \tau_2 = 1/\pi(2) =4
        \end{align*}
        
\end{enumerate}
\end{solution}


\begin{problem}[Practice Exam 6, Problem 2]
    Fix a probability space \( (\Omega,\cF,\PP) \) and a filtration \( \bF = (\cF_t)_{t\geq 0} \). Let \( X \) be an OU process, and \( S \) a strictly increasing L\'evy process (also known as a subordinator), whose dynamics are given by,
    \begin{align*}
        \d X_t = - X_t \d t + \sqrt{2}\d W_t, && \d S_t = \gamma \d t + \int_0^\infty z N(\d t,\d z), && S_0 = 0
    \end{align*}
    where \( W \) is a \( (\PP,\bF) \)-Brownian motion and \( N \) is a poisson random measure with associated L\'evy measure \( \nu \). Consider a Subordianted OU process \( Y \) defined as follows,
    \begin{align*}
        Y_t = X_{S_t}
    \end{align*}
    Define the two-parameter semigroup \( \cP(t,T) \) associated with the \( Y \) process,
    \begin{align*}
        \cP(t,T) f(y) := \EE[f(Y_T) | Y_t = y)
    \end{align*}
    For a fixed \( 0\leq t\leq T <\infty \), what are the eigenfunctions and associated eigenvalues of the operator \( \cP(t,T) \)?
\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item 
    \item 
\end{enumerate}
\end{solution}


\begin{problem}[Practice Exam 7, Problem 1]
\begin{enumerate}[nolistsep,label=(\alph*)]
    \item (Weather chain) The weather can be either sunny, smoggy, or rainy. The weather stays sunny for an exponentially distributed amount of time with mean 3 days and then turns smoggy. It stays smoggy for an exponentially distributed amount of time with mean 4 days and then turns rainy. Finally, it rains for an exponentially distributed amount of time with mean 1 and then it is sunny. Model the weather system as a continuous time Markov chain and compute its stationary distribution.
    \item (Barbershop) Consider a barbershop with one barber and two waiting chairs. The barber cuts hair at rate 3 customers per hour (exponentially distributed hair-cutting time). Customers arrive according to a Poisson process with rate 2 per hour. Arriving customers leave immediately if they find that the two waiting chairs are occupied. Model this system as a continuous time Markov chain and derive its stationary distribution.
\end{enumerate}
\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item Let the state space be \( \{1,2,3\} \) where 1 means sunny, 2 means smoggy, and 3 means rainy.
        By definition of the generator of the Markov process we have generator,
        \begin{align*}
            G = \left[\begin{array}{rrr}
            -1/3 & 1/3 & 0 \\
                0 & -1/4 & 1/4 \\
            1 & 0 & -1
            \end{array}\right]
        \end{align*}
        
        We can easily verify that \( \pi G = 0 \) if,
        \begin{align*}
            \pi = [3/8,1/2,1/8]
        \end{align*} 

    \item 
        Let the state space \( \{0,1,2,3\} \) denote the number of people in the queue. \note{does 2 waiting chairs mean there is also a cutting chair?}
        By definition of the generator of a Markov process we have generator,
        \begin{align*}
            G= \left[\begin{array}{rrrr}
                -2 & 2 \\
                3 & -(3+2) & 2 \\
                & 3 & -(3+2) & 2 \\
                & & 3 & -3
            \end{array}\right]
        \end{align*}

        We can easily verify that \( \pi G = 0 \) if,
        \begin{align*}
            \pi = [1/4,1/4,1/4,1/4]
        \end{align*}     


\end{enumerate}
\end{solution}


\begin{problem}[Practice Exam 7, Problem 2]
    Fix a probability space \( (\Omega,\cF,\PP) \) and a filtration \( \bF = (\cF_t)_{0\leq t\leq T} \). Consider two processes,
    \begin{align*}
        \d X_t = \sigma \d W_t, && \d S_t = \sigma_t S_t \d W_t
    \end{align*}
    where \( W \) is a \( (\PP,\bF) \)-Brownian motino and the process \( \sigma = (\sigma_t)_{0\leq t\leq T} \) evolves independently of \( W \).
\begin{enumerate}[nolistsep,label=(\alph*)]
    \item Show that,
        \begin{align*}
            \EE[G(X_T-X_t)|\cF_t] = \EE[G(X_t-X_T)|\cF_t]
        \end{align*}
    \item Show that,
        \begin{align*}
            \EE[G(S_T)|\cF_t] = \EE[(S_T/S_t) G(S_t^2/S_T) | \cF_t]
        \end{align*}
        
\end{enumerate}

\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item 
    \item 
\end{enumerate}
\end{solution}


\begin{problem}[Practice Exam 8, Problem 1]
A bakery uses a two-step process to make chocolate cakes. The first step involves baking the cake and the second step involves frosting the cake. Baking takes an exponentially distributed amount of time with rate \( \mu_1 \) . After a cake is baked, it goes to the frosting machine. Frosting takes an exponentially distributed amount of time with rate \( \mu_2 \). The processing times at the oven and the frosting machine are independent random variables. Potential cakes arrive according to a Poisson process at rate \( \lambda \), however, a cake goes to the baking oven only if both the oven and the frosting machine are idle. If any of the two is busy, the cake simply exits the system. We wish to model this system as a continuous-time Markov chain.
\begin{enumerate}[nolistsep,label=(\alph*)]
    \item Precisely define the states of your continuous-time Markov chain (Hint: your model should have three states and note that there will never be two cakes in this system).
    \item Draw a transition diagram for your continuous-time Markov chain.
    \item Derive stationary distribution for your continuous-time Markov chain.
    \item Find the expected number of cakes in this system in steady-state.
\end{enumerate}
\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item 
    \item 
\end{enumerate}
\end{solution}


\begin{problem}[Practice Exam 8, Problem 2]
    Fix a probability space \( (\Omega,\cF,\PP) \) and a filtration \( \bF = (\cF_t)_{0\leq t\leq T} \). Define a procecss \( X \) and stoppting time \( \tau \) as follows,
    \begin{align*}
        \d X_t = \mu \d t + \sigma \d W_t, && \tau = \inf\{t\geq 0: X_t\notin(a,b)\}
    \end{align*}
    where \( W \) is a \( (\PP,\bF) \)-Brownian motion. Define the following Laplace Transform,
    \begin{align*}
        L(x;\lambda) := \EE[e^{-\lambda \tau} | X_0 = 0), && x\in(a,b)
    \end{align*}
    Derive the PDE satisfied by \( L \) and use this to find \( L \) explicitly.
\end{problem}

\begin{solution}[Solution]
\begin{enumerate}[label=(\alph*)]
    \item 
    \item 
\end{enumerate}
\end{solution}



